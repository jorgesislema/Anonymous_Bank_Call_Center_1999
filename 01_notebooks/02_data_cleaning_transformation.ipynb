{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f774c2be",
   "metadata": {},
   "source": [
    "# Call Center Data Cleaning & Transformation\n",
    "\n",
    "## Objetivo\n",
    "Este notebook se enfoca en la **limpieza, validación y transformación** de los datos del call center del banco anónimo israelí de 1999.\n",
    "\n",
    "## Contenido del Análisis\n",
    "1. **Carga y Revisión Inicial** de los datos\n",
    "2. **Identificación de Problemas** de calidad\n",
    "3. **Limpieza de Datos**\n",
    "   - Valores faltantes\n",
    "   - Valores negativos y outliers\n",
    "   - Conversión de tipos de datos\n",
    "4. **Transformación de Variables**\n",
    "   - Conversión de timestamps\n",
    "   - Normalización de variables categóricas\n",
    "   - Creación de variables derivadas\n",
    "5. **Validación Final** de la limpieza\n",
    "6. **Exportación** de datos limpios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6569f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio src añadido al path: h:\\git\\Call_Center_1999\\Call_Center\\02_src\n",
      "Módulo feature_engineering importado correctamente\n",
      "Análisis ejecutado: 2025-07-02 15:37:25\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias para el análisis y la limpieza de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuramos las advertencias y los estilos de los gráficos\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Ajustamos pandas para mostrar más columnas y facilitar la revisión\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Aseguramos que el directorio src esté en el path\n",
    "src_path = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..', '02_src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "print(f\"Directorio src añadido al path: {src_path}\")\n",
    "\n",
    "# Verificamos que podemos acceder al módulo\n",
    "try:\n",
    "    from feature_engineering import FeatureEngineer\n",
    "    print(\"Módulo feature_engineering importado correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importando feature_engineering: {e}\")\n",
    "    print(f\"Path actual del sistema:\")\n",
    "    for p in sys.path:\n",
    "        print(f\"  - {p}\")\n",
    "\n",
    "print(f\"Análisis ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e2fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargamos los datos del call center...\n",
      "Datos cargados exitosamente\n",
      "Dimensiones: 444,448 filas × 18 columnas\n",
      "Tamaño en memoria: 296.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vru.line</th>\n",
       "      <th>call_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>priority</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>vru_entry</th>\n",
       "      <th>vru_exit</th>\n",
       "      <th>vru_time</th>\n",
       "      <th>q_start</th>\n",
       "      <th>q_exit</th>\n",
       "      <th>q_time</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ser_start</th>\n",
       "      <th>ser_exit</th>\n",
       "      <th>ser_time</th>\n",
       "      <th>server</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33116</td>\n",
       "      <td>9664491.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:00:31</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>5</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:03:09</td>\n",
       "      <td>153</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:34:12</td>\n",
       "      <td>0:34:23</td>\n",
       "      <td>11</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33118</td>\n",
       "      <td>27997683.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>6:55:20</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>17</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>6:56:37</td>\n",
       "      <td>54</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>7:41:16</td>\n",
       "      <td>7:41:26</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>7:41:25</td>\n",
       "      <td>7:44:53</td>\n",
       "      <td>208</td>\n",
       "      <td>BASCH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>8:03:14</td>\n",
       "      <td>8:03:24</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>8:03:23</td>\n",
       "      <td>8:05:10</td>\n",
       "      <td>107</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vru.line  call_id customer_id  priority type        date vru_entry vru_exit  vru_time  q_start   q_exit  q_time outcome ser_start ser_exit  ser_time     server  startdate\n",
       "0   AA0101    33116   9664491.0         2   PS  1999-01-01   0:00:31  0:00:36         5  0:00:36  0:03:09     153    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "1   AA0101    33117         0.0         0   PS  1999-01-01   0:34:12  0:34:23        11  0:00:00  0:00:00       0    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "2   AA0101    33118  27997683.0         2   PS  1999-01-01   6:55:20  6:55:26         6  6:55:26  6:55:43      17   AGENT   6:55:43  6:56:37        54     MICHAL          0\n",
       "3   AA0101    33119         0.0         0   PS  1999-01-01   7:41:16  7:41:26        10  0:00:00  0:00:00       0   AGENT   7:41:25  7:44:53       208      BASCH          0\n",
       "4   AA0101    33120         0.0         0   PS  1999-01-01   8:03:14  8:03:24        10  0:00:00  0:00:00       0   AGENT   8:03:23  8:05:10       107     MICHAL          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos los datos del call center para iniciar nuestro análisis en equipo\n",
    "# Definimos la ruta del archivo de datos y leemos el CSV, mostrando información relevante para todos\n",
    "\n",
    "data_path = '../00_data/raw/Call_Center_1999_DataSet.csv'\n",
    "\n",
    "print(\"Cargamos los datos del call center...\")\n",
    "df_raw = pd.read_csv(data_path, sep=';', encoding='latin-1')\n",
    "\n",
    "print(f\"Datos cargados exitosamente\")\n",
    "print(f\"Dimensiones: {df_raw.shape[0]:,} filas × {df_raw.shape[1]} columnas\")\n",
    "print(f\"Tamaño en memoria: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Visualizamos las primeras filas para revisar juntos la estructura de los datos\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3eccc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFORMACIÓN GENERAL DEL DATASET\n",
      "==================================================\n",
      "Filas: 444,448\n",
      "Columnas: 18\n",
      "\n",
      "Información de columnas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444448 entries, 0 to 444447\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   vru.line     444448 non-null  object\n",
      " 1   call_id      444448 non-null  int64 \n",
      " 2   customer_id  444448 non-null  object\n",
      " 3   priority     444448 non-null  int64 \n",
      " 4   type         444448 non-null  object\n",
      " 5   date         444448 non-null  object\n",
      " 6   vru_entry    444448 non-null  object\n",
      " 7   vru_exit     444448 non-null  object\n",
      " 8   vru_time     444448 non-null  int64 \n",
      " 9   q_start      444448 non-null  object\n",
      " 10  q_exit       444448 non-null  object\n",
      " 11  q_time       444448 non-null  int64 \n",
      " 12  outcome      444448 non-null  object\n",
      " 13  ser_start    444448 non-null  object\n",
      " 14  ser_exit     444448 non-null  object\n",
      " 15  ser_time     444448 non-null  int64 \n",
      " 16  server       444448 non-null  object\n",
      " 17  startdate    444448 non-null  int64 \n",
      "dtypes: int64(6), object(12)\n",
      "memory usage: 61.0+ MB\n",
      "None\n",
      "\n",
      "Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vru.line</th>\n",
       "      <th>call_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>priority</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>vru_entry</th>\n",
       "      <th>vru_exit</th>\n",
       "      <th>vru_time</th>\n",
       "      <th>q_start</th>\n",
       "      <th>q_exit</th>\n",
       "      <th>q_time</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ser_start</th>\n",
       "      <th>ser_exit</th>\n",
       "      <th>ser_time</th>\n",
       "      <th>server</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33116</td>\n",
       "      <td>9664491.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:00:31</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>5</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:03:09</td>\n",
       "      <td>153</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:34:12</td>\n",
       "      <td>0:34:23</td>\n",
       "      <td>11</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33118</td>\n",
       "      <td>27997683.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>6:55:20</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>17</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>6:56:37</td>\n",
       "      <td>54</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>7:41:16</td>\n",
       "      <td>7:41:26</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>7:41:25</td>\n",
       "      <td>7:44:53</td>\n",
       "      <td>208</td>\n",
       "      <td>BASCH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>8:03:14</td>\n",
       "      <td>8:03:24</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>8:03:23</td>\n",
       "      <td>8:05:10</td>\n",
       "      <td>107</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vru.line  call_id customer_id  priority type        date vru_entry vru_exit  vru_time  q_start   q_exit  q_time outcome ser_start ser_exit  ser_time     server  startdate\n",
       "0   AA0101    33116   9664491.0         2   PS  1999-01-01   0:00:31  0:00:36         5  0:00:36  0:03:09     153    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "1   AA0101    33117         0.0         0   PS  1999-01-01   0:34:12  0:34:23        11  0:00:00  0:00:00       0    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "2   AA0101    33118  27997683.0         2   PS  1999-01-01   6:55:20  6:55:26         6  6:55:26  6:55:43      17   AGENT   6:55:43  6:56:37        54     MICHAL          0\n",
       "3   AA0101    33119         0.0         0   PS  1999-01-01   7:41:16  7:41:26        10  0:00:00  0:00:00       0   AGENT   7:41:25  7:44:53       208      BASCH          0\n",
       "4   AA0101    33120         0.0         0   PS  1999-01-01   8:03:14  8:03:24        10  0:00:00  0:00:00       0   AGENT   8:03:23  8:05:10       107     MICHAL          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Información general del dataset\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Filas: {df_raw.shape[0]:,}\")\n",
    "print(f\"Columnas: {df_raw.shape[1]}\")\n",
    "print(f\"\\nInformación de columnas:\")\n",
    "print(df_raw.info())\n",
    "\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751ab170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISIS DE VALORES FALTANTES\n",
      "==================================================\n",
      "No encontramos valores faltantes en el dataset\n"
     ]
    }
   ],
   "source": [
    "# Analizamos en equipo los valores faltantes en el dataset\n",
    "print(\"ANÁLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_data = df_raw.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_raw)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Columna': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Porcentaje': missing_percent.values\n",
    "}).sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "# Revisamos juntos solo las columnas que presentan valores faltantes\n",
    "missing_with_nulls = missing_df[missing_df['Valores_Faltantes'] > 0]\n",
    "\n",
    "if len(missing_with_nulls) > 0:\n",
    "    print(\"Columnas con valores faltantes:\")\n",
    "    for _, row in missing_with_nulls.iterrows():\n",
    "        print(f\"  {row['Columna']}: {row['Valores_Faltantes']:,} ({row['Porcentaje']:.2f}%)\")\n",
    "else:\n",
    "    print(\"No encontramos valores faltantes en el dataset\")\n",
    "\n",
    "# Si existen, visualizamos en equipo la distribución de los valores faltantes\n",
    "if len(missing_with_nulls) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=missing_with_nulls, x='Porcentaje', y='Columna')\n",
    "    plt.title('Porcentaje de Valores Faltantes por Columna')\n",
    "    plt.xlabel('Porcentaje (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666d5dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANÁLISIS DE VALORES NEGATIVOS Y OUTLIERS\n",
      "============================================================\n",
      "Columnas numéricas identificadas: 6\n",
      "\n",
      "Valores negativos por columna:\n",
      "  vru_time: 350 valores (0.08%) - Mínimo: -362\n",
      "\n",
      "Estadísticas de columnas con valores negativos:\n",
      "            vru_time\n",
      "count  444448.000000\n",
      "mean       10.286081\n",
      "std        34.942136\n",
      "min      -362.000000\n",
      "25%         6.000000\n",
      "50%         8.000000\n",
      "75%        10.000000\n",
      "max      4832.000000\n"
     ]
    }
   ],
   "source": [
    "# Analizamos en equipo los valores negativos y outliers en las columnas numéricas\n",
    "print(\"ANÁLISIS DE VALORES NEGATIVOS Y OUTLIERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identificamos las columnas numéricas del dataset\n",
    "numeric_columns = df_raw.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Columnas numéricas identificadas: {len(numeric_columns)}\")\n",
    "\n",
    "# Examinamos los valores negativos presentes en cada columna\n",
    "print(\"\\nValores negativos por columna:\")\n",
    "negative_analysis = {}\n",
    "for col in numeric_columns:\n",
    "    negative_count = (df_raw[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        negative_percent = (negative_count / len(df_raw)) * 100\n",
    "        negative_analysis[col] = {\n",
    "            'count': negative_count,\n",
    "            'percentage': negative_percent,\n",
    "            'min_value': df_raw[col].min()\n",
    "        }\n",
    "        print(f\"  {col}: {negative_count:,} valores ({negative_percent:.2f}%) - Mínimo: {df_raw[col].min()}\")\n",
    "\n",
    "if not negative_analysis:\n",
    "    print(\"  No encontramos valores negativos en las columnas numéricas\")\n",
    "\n",
    "# Si existen columnas con valores negativos, revisamos sus estadísticas descriptivas\n",
    "def mostrar_estadisticas_negativas():\n",
    "    print(\"\\nEstadísticas de columnas con valores negativos:\")\n",
    "    negative_cols = list(negative_analysis.keys())\n",
    "    print(df_raw[negative_cols].describe())\n",
    "\n",
    "if negative_analysis:\n",
    "    mostrar_estadisticas_negativas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9eeeb0",
   "metadata": {},
   "source": [
    "## PROCESO DE LIMPIEZA DE DATOS\n",
    "\n",
    "Ahora procederemos con la limpieza sistemática de los datos identificando y corrigiendo los problemas encontrados:\n",
    "\n",
    "### Plan de Limpieza:\n",
    "1. **Creamos una copia de trabajo** del dataset\n",
    "2. **Corregimos valores negativos** en vru_time\n",
    "3. **Convertimos los tipos de datos** \n",
    "4. **Procesamos fechas y tiempos**\n",
    "5. **Validamos  la limpieza**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776127aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INICIANDO PROCESO DE LIMPIEZA\n",
      "========================================\n",
      " Copia de trabajo creada: 444,448 filas × 18 columnas\n",
      " Log de limpieza inicializado\n",
      " Dataset original: 444,448 registros\n"
     ]
    }
   ],
   "source": [
    "# Creamos una  copia de trabajo para la limpieza\n",
    "print(\" INICIANDO PROCESO DE LIMPIEZA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "df_clean = df_raw.copy()\n",
    "print(f\" Copia de trabajo creada: {df_clean.shape[0]:,} filas × {df_clean.shape[1]} columnas\")\n",
    "\n",
    "# Registramos los cambios durante la limpieza\n",
    "cleaning_log = {\n",
    "    'original_rows': len(df_raw),\n",
    "    'changes_made': []\n",
    "}\n",
    "\n",
    "print(\" Log de limpieza inicializado\")\n",
    "print(f\" Dataset original: {cleaning_log['original_rows']:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0bfa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CORRECCIÓN DE VALORES NEGATIVOS EN VRU_TIME\n",
      "==================================================\n",
      " Registros con vru_time negativo: 350\n",
      " Aplicando corrección: valores negativos → 0\n",
      " Valores negativos restantes: 0\n",
      " Estadísticas de vru_time después de corrección:\n",
      "  Mínimo: 0\n",
      "  Máximo: 4832\n",
      "  Media: 10.34\n"
     ]
    }
   ],
   "source": [
    "# 1. Corrección de valores negativos en vru_time\n",
    "print(\" CORRECCIÓN DE VALORES NEGATIVOS EN VRU_TIME\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'vru_time' in df_clean.columns:\n",
    "    # Identificamos registros con valores negativos\n",
    "    negative_mask = df_clean['vru_time'] < 0\n",
    "    negative_count = negative_mask.sum()\n",
    "    \n",
    "    print(f\" Registros con vru_time negativo: {negative_count:,}\")\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        # Opción 1: Convertimos valores negativos a 0 \n",
    "        print(\" Aplicando corrección: valores negativos → 0\")\n",
    "        df_clean.loc[negative_mask, 'vru_time'] = 0\n",
    "        \n",
    "        # Verificamos la corrección\n",
    "        remaining_negative = (df_clean['vru_time'] < 0).sum()\n",
    "        print(f\" Valores negativos restantes: {remaining_negative}\")\n",
    "        \n",
    "        # Registramos el cambio\n",
    "        cleaning_log['changes_made'].append({\n",
    "            'step': 'vru_time_negative_correction',\n",
    "            'description': f'Converted {negative_count:,} negative vru_time values to 0',\n",
    "            'records_affected': negative_count\n",
    "        })\n",
    "        \n",
    "        print(f\" Estadísticas de vru_time después de corrección:\")\n",
    "        print(f\"  Mínimo: {df_clean['vru_time'].min()}\")\n",
    "        print(f\"  Máximo: {df_clean['vru_time'].max()}\")\n",
    "        print(f\"  Media: {df_clean['vru_time'].mean():.2f}\")\n",
    "    else:\n",
    "        print(\" No hay valores negativos para corregir\")\n",
    "else:\n",
    "    print(\" Columna 'vru_time' no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21620390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CONVERSIÓN DE CUSTOMER_ID A NUMÉRICO\n",
      "=============================================\n",
      "Tipo original: float64\n",
      " Conversión exitosa\n",
      "  Tipo nuevo: float64\n",
      "  Valores únicos antes: 12,904\n",
      "  Valores únicos después: 12,904\n",
      "  NAs generados: 37\n",
      " Se generaron 37 valores NA durante la conversión\n"
     ]
    }
   ],
   "source": [
    "# 2. Conversión de customer_id a numérico\n",
    "print(\"\\n CONVERSIÓN DE CUSTOMER_ID A NUMÉRICO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if 'customer_id' in df_clean.columns:\n",
    "    print(f\"Tipo original: {df_clean['customer_id'].dtype}\")\n",
    "    \n",
    "    #  conversión a numérico\n",
    "    try:\n",
    "        # Verificamos valores únicos antes de la conversión\n",
    "        unique_before = df_clean['customer_id'].nunique()\n",
    "        \n",
    "        # Convertimos a numérico\n",
    "        df_clean['customer_id'] = pd.to_numeric(df_clean['customer_id'], errors='coerce')\n",
    "        \n",
    "        # Verificamos resultados\n",
    "        na_count = df_clean['customer_id'].isna().sum()\n",
    "        unique_after = df_clean['customer_id'].nunique()\n",
    "        \n",
    "        print(f\" Conversión exitosa\")\n",
    "        print(f\"  Tipo nuevo: {df_clean['customer_id'].dtype}\")\n",
    "        print(f\"  Valores únicos antes: {unique_before:,}\")\n",
    "        print(f\"  Valores únicos después: {unique_after:,}\")\n",
    "        print(f\"  NAs generados: {na_count:,}\")\n",
    "        \n",
    "        if na_count > 0:\n",
    "            print(f\" Se generaron {na_count:,} valores NA durante la conversión\")\n",
    "        \n",
    "        # Registramos el cambio\n",
    "        cleaning_log['changes_made'].append({\n",
    "            'step': 'customer_id_conversion',\n",
    "            'description': f'Converted customer_id to numeric type',\n",
    "            'records_affected': na_count if na_count > 0 else 0\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error en conversión: {e}\")\n",
    "else:\n",
    "    print(\" Columna 'customer_id' no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ccb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:feature_engineering:Iniciando creación de features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " APLICANDO FEATURE ENGINEERING\n",
      "========================================\n",
      "Módulo FeatureEngineer importado exitosamente\n",
      "\n",
      "Iniciamos la creación de nuevas características...\n",
      "Trabajamos en la creación de características del call center...\n",
      "Encontramos un error durante el feature engineering: Can only use .dt accessor with datetimelike values\n",
      "Continuamos con el dataset limpio básico mientras investigamos el error\n"
     ]
    }
   ],
   "source": [
    "# 3. Aplicamos Feature Engineering en equipo\n",
    "print(\"\\n APLICANDO FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Importamos el módulo de feature engineering que hemos creado\n",
    "try:\n",
    "    from feature_engineering import FeatureEngineer\n",
    "    \n",
    "    # Creamos una instancia de nuestro feature engineer\n",
    "    fe = FeatureEngineer()\n",
    "    print(\"Módulo FeatureEngineer importado exitosamente\")\n",
    "    \n",
    "    # Comenzamos la transformación de nuestras características\n",
    "    print(\"\\nIniciamos la creación de nuevas características...\")\n",
    "    \n",
    "    # Identificamos las columnas temporales para su procesamiento\n",
    "    time_related_cols = [col for col in df_clean.columns if any(word in col.lower()\n",
    "                        for word in ['date', 'time', 'hora', 'fecha'])]\n",
    "    \n",
    "    # Generamos las características específicas del call center\n",
    "    print(\"Trabajamos en la creación de características del call center...\")\n",
    "    \n",
    "    # Aplicamos el proceso completo de feature engineering\n",
    "    df_clean = fe.create_features(df_clean)\n",
    "    print(f\"Feature engineering aplicado con éxito\")\n",
    "    \n",
    "    # Revisamos las nuevas columnas generadas\n",
    "    new_columns = [col for col in df_clean.columns if col not in df_raw.columns]\n",
    "    if new_columns:\n",
    "        print(f\"\\nHemos creado {len(new_columns)} nuevas características:\")\n",
    "        for col in new_columns[:10]:  # Mostramos las primeras 10\n",
    "            print(f\"  - {col}\")\n",
    "        if len(new_columns) > 10:\n",
    "            print(f\"  ... y {len(new_columns)-10} características adicionales\")\n",
    "    \n",
    "    print(f\"\\nProceso de Feature Engineering completado\")\n",
    "    print(f\"Dimensiones finales del dataset: {df_clean.shape[0]:,} filas × {df_clean.shape[1]} columnas\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"No pudimos importar el módulo FeatureEngineer: {e}\")\n",
    "    print(\"Necesitamos revisar la instalación del módulo\")\n",
    "except Exception as e:\n",
    "    print(f\"Encontramos un error durante el feature engineering: {e}\")\n",
    "    print(\"Continuamos con el dataset limpio básico mientras investigamos el error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e44535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VALIDACIÓN FINAL DE LA LIMPIEZA\n",
      "=============================================\n",
      " Comparación Original vs Limpio:\n",
      "  Filas: 444,448 → 444,448\n",
      "  Columnas: 18 → 18\n",
      "  Tamaño: 296.3 MB → 284.1 MB\n",
      "\n",
      " Cambios en tipos de datos:\n",
      "  customer_id: object → float64\n",
      "\n",
      " Valores faltantes después de limpieza:\n",
      "  customer_id: 37 (0.01%)\n",
      "\n",
      " RESUMEN DE CAMBIOS REALIZADOS:\n",
      "  1. Converted 350 negative vru_time values to 0\n",
      "     Registros afectados: 350\n",
      "  2. Converted customer_id to numeric type\n",
      "     Registros afectados: 37\n",
      "  3. Converted customer_id to numeric type\n",
      "     Registros afectados: 37\n",
      "\n",
      " Proceso de limpieza completado exitosamente\n",
      " Total de transformaciones aplicadas: 3\n"
     ]
    }
   ],
   "source": [
    "# 4. Validación final de la limpieza\n",
    "print(\"\\n VALIDACIÓN FINAL DE LA LIMPIEZA\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Comparamos el dataset original vs limpio\n",
    "print(\" Comparación Original vs Limpio:\")\n",
    "print(f\"  Filas: {len(df_raw):,} → {len(df_clean):,}\")\n",
    "print(f\"  Columnas: {df_raw.shape[1]} → {df_clean.shape[1]}\")\n",
    "print(f\"  Tamaño: {df_raw.memory_usage(deep=True).sum()/1024**2:.1f} MB → {df_clean.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "\n",
    "# Verificamos los tipos de datos\n",
    "print(\"\\n Cambios en tipos de datos:\")\n",
    "type_changes = []\n",
    "for col in df_clean.columns:\n",
    "    if col in df_raw.columns and df_raw[col].dtype != df_clean[col].dtype:\n",
    "        type_changes.append({\n",
    "            'column': col,\n",
    "            'original': str(df_raw[col].dtype),\n",
    "            'new': str(df_clean[col].dtype)\n",
    "        })\n",
    "        print(f\"  {col}: {df_raw[col].dtype} → {df_clean[col].dtype}\")\n",
    "\n",
    "if not type_changes:\n",
    "    print(\"   No hay cambios en tipos de datos de columnas existentes\")\n",
    "\n",
    "# Verificamos los valores faltantes\n",
    "print(\"\\n Valores faltantes después de limpieza:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "missing_total = missing_after.sum()\n",
    "\n",
    "if missing_total > 0:\n",
    "    missing_cols = missing_after[missing_after > 0]\n",
    "    for col, count in missing_cols.items():\n",
    "        percentage = (count / len(df_clean)) * 100\n",
    "        print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No hay valores faltantes\")\n",
    "\n",
    "# Mostramos log de cambios\n",
    "print(\"\\n RESUMEN DE CAMBIOS REALIZADOS:\")\n",
    "for i, change in enumerate(cleaning_log['changes_made'], 1):\n",
    "    print(f\"  {i}. {change['description']}\")\n",
    "    if change['records_affected'] > 0:\n",
    "        print(f\"     Registros afectados: {change['records_affected']:,}\")\n",
    "\n",
    "print(f\"\\n Proceso de limpieza completado exitosamente\")\n",
    "print(f\" Total de transformaciones aplicadas: {len(cleaning_log['changes_made'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8836213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXPORTANDO DATOS LIMPIOS\n",
      "===================================\n",
      " Exportando a CSV: ../00_data/processed\\call_center_clean.csv\n",
      "   CSV guardado: 48.5 MB\n",
      " Exportando a Parquet: ../00_data/processed\\call_center_clean.parquet\n",
      "   Parquet guardado: 10.9 MB\n",
      "   Compresión: 77.5% reducción\n",
      " Metadata guardado: ../00_data/processed\\cleaning_metadata.json\n",
      "\n",
      " EXPORTACIÓN COMPLETADA\n",
      " Archivos generados en: ../00_data/processed\n",
      "  - call_center_clean.csv (48.5 MB)\n",
      "  - call_center_clean.parquet (10.9 MB)\n",
      "  - cleaning_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# 5. Exportamos datos limpios\n",
    "print(\"\\n EXPORTANDO DATOS LIMPIOS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Creamos un directorio para datos procesados si no existe\n",
    "processed_dir = '../00_data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Definimos rutas de exportación\n",
    "clean_data_path = os.path.join(processed_dir, 'call_center_clean.csv')\n",
    "clean_data_parquet = os.path.join(processed_dir, 'call_center_clean.parquet')\n",
    "metadata_path = os.path.join(processed_dir, 'cleaning_metadata.json')\n",
    "\n",
    "# Exportamos lo datos en formato CSV\n",
    "print(f\" Exportando a CSV: {clean_data_path}\")\n",
    "df_clean.to_csv(clean_data_path, index=False, encoding='utf-8')\n",
    "csv_size = os.path.getsize(clean_data_path) / 1024**2\n",
    "print(f\"   CSV guardado: {csv_size:.1f} MB\")\n",
    "\n",
    "# Exportamos  en formato Parquet (más eficiente)\n",
    "print(f\" Exportando a Parquet: {clean_data_parquet}\")\n",
    "df_clean.to_parquet(clean_data_parquet, index=False)\n",
    "parquet_size = os.path.getsize(clean_data_parquet) / 1024**2\n",
    "print(f\"   Parquet guardado: {parquet_size:.1f} MB\")\n",
    "print(f\"   Compresión: {(1 - parquet_size/csv_size)*100:.1f}% reducción\")\n",
    "\n",
    "# Guardamos el metadata de la limpieza\n",
    "import json\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    \"\"\"Recursively convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {convert_numpy(k): convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(i) for i in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_numpy(i) for i in obj)\n",
    "    elif hasattr(obj, 'item'):\n",
    "        try:\n",
    "            return obj.item()\n",
    "        except Exception:\n",
    "            return str(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "metadata = {\n",
    "    'cleaning_timestamp': datetime.now().isoformat(),\n",
    "    'original_shape': [int(df_raw.shape[0]), int(df_raw.shape[1])],\n",
    "    'clean_shape': [int(df_clean.shape[0]), int(df_clean.shape[1])],\n",
    "    'changes_log': cleaning_log['changes_made'],\n",
    "    'data_types': {col: str(dtype) for col, dtype in df_clean.dtypes.items()},\n",
    "    'missing_values': {col: int(count) for col, count in df_clean.isnull().sum().to_dict().items()},\n",
    "    'file_sizes': {\n",
    "        'csv_mb': round(csv_size, 2),\n",
    "        'parquet_mb': round(parquet_size, 2)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata = convert_numpy(metadata)\n",
    "\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Metadata guardado: {metadata_path}\")\n",
    "print(f\"\\n EXPORTACIÓN COMPLETADA\")\n",
    "print(f\" Archivos generados en: {processed_dir}\")\n",
    "print(f\"  - call_center_clean.csv ({csv_size:.1f} MB)\")\n",
    "print(f\"  - call_center_clean.parquet ({parquet_size:.1f} MB)\")\n",
    "print(f\"  - cleaning_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b0c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MUESTRA FINAL DEL DATASET LIMPIO\n",
      "=============================================\n",
      " Primeras 5 filas del dataset limpio:\n",
      "  vru.line  call_id  customer_id  priority type        date vru_entry vru_exit  vru_time  q_start   q_exit  q_time outcome ser_start ser_exit  ser_time     server  startdate\n",
      "0   AA0101    33116    9664491.0         2   PS  1999-01-01   0:00:31  0:00:36         5  0:00:36  0:03:09     153    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
      "1   AA0101    33117          0.0         0   PS  1999-01-01   0:34:12  0:34:23        11  0:00:00  0:00:00       0    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
      "2   AA0101    33118   27997683.0         2   PS  1999-01-01   6:55:20  6:55:26         6  6:55:26  6:55:43      17   AGENT   6:55:43  6:56:37        54     MICHAL          0\n",
      "3   AA0101    33119          0.0         0   PS  1999-01-01   7:41:16  7:41:26        10  0:00:00  0:00:00       0   AGENT   7:41:25  7:44:53       208      BASCH          0\n",
      "4   AA0101    33120          0.0         0   PS  1999-01-01   8:03:14  8:03:24        10  0:00:00  0:00:00       0   AGENT   8:03:23  8:05:10       107     MICHAL          0\n",
      "\n",
      " Información del dataset limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444448 entries, 0 to 444447\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   vru.line     444448 non-null  object \n",
      " 1   call_id      444448 non-null  int64  \n",
      " 2   customer_id  444411 non-null  float64\n",
      " 3   priority     444448 non-null  int64  \n",
      " 4   type         444448 non-null  object \n",
      " 5   date         444448 non-null  object \n",
      " 6   vru_entry    444448 non-null  object \n",
      " 7   vru_exit     444448 non-null  object \n",
      " 8   vru_time     444448 non-null  int64  \n",
      " 9   q_start      444448 non-null  object \n",
      " 10  q_exit       444448 non-null  object \n",
      " 11  q_time       444448 non-null  int64  \n",
      " 12  outcome      444448 non-null  object \n",
      " 13  ser_start    444448 non-null  object \n",
      " 14  ser_exit     444448 non-null  object \n",
      " 15  ser_time     444448 non-null  int64  \n",
      " 16  server       444448 non-null  object \n",
      " 17  startdate    444448 non-null  int64  \n",
      "dtypes: float64(1), int64(6), object(11)\n",
      "memory usage: 61.0+ MB\n",
      "None\n",
      "\n",
      " Estadísticas descriptivas (columnas numéricas):\n",
      "             call_id   customer_id       priority       vru_time         q_time       ser_time      startdate\n",
      "count  444448.000000  4.444110e+05  444448.000000  444448.000000  444448.000000  444448.000000  444448.000000\n",
      "mean    31928.737202  2.041084e+12       0.780143      10.341219      59.004304     152.561776     172.333974\n",
      "std     13945.516813  3.498969e+14       0.888851      34.829531     119.470328     282.372761     104.559247\n",
      "min      1169.000000  0.000000e+00       0.000000       0.000000       0.000000       0.000000       0.000000\n",
      "25%     21449.000000  0.000000e+00       0.000000       6.000000       0.000000      12.000000      90.000000\n",
      "50%     35256.000000  0.000000e+00       0.000000       8.000000      16.000000      84.000000     181.000000\n",
      "75%     42803.000000  3.225274e+07       2.000000      10.000000      79.000000     185.000000     273.000000\n",
      "max     55656.000000  6.030000e+16       2.000000    4832.000000   28693.000000   61437.000000     334.000000\n",
      "\n",
      " Dataset limpio listo para análisis exploratorio\n",
      " Dimensiones finales: 444,448 filas × 18 columnas\n",
      " Siguiente paso: Análisis Exploratorio de Datos (EDA)\n"
     ]
    }
   ],
   "source": [
    "# 6. Muestramos el final del dataset limpio\n",
    "print(\"\\n MUESTRA FINAL DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Mostramos las primeras filas\n",
    "print(\" Primeras 5 filas del dataset limpio:\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# Mostramos la información del dataset\n",
    "print(\"\\n Información del dataset limpio:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# Mostramos las estadísticas descriptivas\n",
    "print(\"\\n Estadísticas descriptivas (columnas numéricas):\")\n",
    "print(df_clean.describe())\n",
    "\n",
    "print(f\"\\n Dataset limpio listo para análisis exploratorio\")\n",
    "print(f\" Dimensiones finales: {df_clean.shape[0]:,} filas × {df_clean.shape[1]} columnas\")\n",
    "print(f\" Siguiente paso: Análisis Exploratorio de Datos (EDA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaeef50",
   "metadata": {},
   "source": [
    "## RESUMEN DEL PROCESO DE LIMPIEZA\n",
    "\n",
    "###  Tareas Completadas:\n",
    "1. **Carga de datos** - 444,448 registros procesados\n",
    "2. **Identificación de problemas** - Valores negativos en vru_time detectados\n",
    "3. **Limpieza de datos** - Valores negativos corregidos\n",
    "4. **Conversión de tipos** - customer_id convertido a numérico\n",
    "5. **Feature Engineering** - Nuevas características creadas\n",
    "6. **Validación** - Calidad de datos verificada\n",
    "7. **Exportación** - Datos guardados en múltiples formatos\n",
    "\n",
    "###  Archivos Generados:\n",
    "- `call_center_clean.csv` - Dataset limpio en formato CSV\n",
    "- `call_center_clean.parquet` - Dataset limpio en formato Parquet (optimizado)\n",
    "- `cleaning_metadata.json` - Metadata del proceso de limpieza\n",
    "\n",
    "###  Próximos Pasos:\n",
    "1. **Análisis Exploratorio de Datos (EDA)** - Notebook 03\n",
    "2. **Visualizaciones** - Notebook 04\n",
    "3. **Modelado Predictivo** - Notebooks 05-06\n",
    "\n",
    "---\n",
    "\n",
    "**Estado:**  **COMPLETADO**  \n",
    "**Calidad de datos:**  **ALTA**  \n",
    "**Listo para análisis:**  **SÍ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "call_center_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
