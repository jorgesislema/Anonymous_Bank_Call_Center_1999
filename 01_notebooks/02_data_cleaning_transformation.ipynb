{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f774c2be",
   "metadata": {},
   "source": [
    "# Call Center Data Cleaning & Transformation\n",
    "\n",
    "## Objetivo\n",
    "Este notebook se enfoca en la **limpieza, validaci√≥n y transformaci√≥n** de los datos del call center del banco an√≥nimo israel√≠ de 1999.\n",
    "\n",
    "## Contenido del An√°lisis\n",
    "1. **Carga y Revisi√≥n Inicial** de los datos\n",
    "2. **Identificaci√≥n de Problemas** de calidad\n",
    "3. **Limpieza de Datos**\n",
    "   - Valores faltantes\n",
    "   - Valores negativos y outliers\n",
    "   - Conversi√≥n de tipos de datos\n",
    "4. **Transformaci√≥n de Variables**\n",
    "   - Conversi√≥n de timestamps\n",
    "   - Normalizaci√≥n de variables categ√≥ricas\n",
    "   - Creaci√≥n de variables derivadas\n",
    "5. **Validaci√≥n Final** de la limpieza\n",
    "6. **Exportaci√≥n** de datos limpios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6569f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n",
      "üìÖ An√°lisis ejecutado: 2025-05-23 18:58:37\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configurar pandas para mostrar m√°s columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Agregar directorio src al path\n",
    "sys.path.append('../02_src')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìÖ An√°lisis ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e2fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cargando datos del call center...\n",
      "‚úÖ Datos cargados exitosamente\n",
      "üìä Dimensiones: 444,448 filas √ó 18 columnas\n",
      "üíæ Tama√±o en memoria: 296.3 MB\n",
      "\n",
      "üîç Primeras 5 filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vru.line</th>\n",
       "      <th>call_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>priority</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>vru_entry</th>\n",
       "      <th>vru_exit</th>\n",
       "      <th>vru_time</th>\n",
       "      <th>q_start</th>\n",
       "      <th>q_exit</th>\n",
       "      <th>q_time</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ser_start</th>\n",
       "      <th>ser_exit</th>\n",
       "      <th>ser_time</th>\n",
       "      <th>server</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33116</td>\n",
       "      <td>9664491.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:00:31</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>5</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:03:09</td>\n",
       "      <td>153</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:34:12</td>\n",
       "      <td>0:34:23</td>\n",
       "      <td>11</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33118</td>\n",
       "      <td>27997683.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>6:55:20</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>17</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>6:56:37</td>\n",
       "      <td>54</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>7:41:16</td>\n",
       "      <td>7:41:26</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>7:41:25</td>\n",
       "      <td>7:44:53</td>\n",
       "      <td>208</td>\n",
       "      <td>BASCH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>8:03:14</td>\n",
       "      <td>8:03:24</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>8:03:23</td>\n",
       "      <td>8:05:10</td>\n",
       "      <td>107</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vru.line  call_id customer_id  priority type        date vru_entry vru_exit  vru_time  q_start   q_exit  q_time outcome ser_start ser_exit  ser_time     server  startdate\n",
       "0   AA0101    33116   9664491.0         2   PS  1999-01-01   0:00:31  0:00:36         5  0:00:36  0:03:09     153    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "1   AA0101    33117         0.0         0   PS  1999-01-01   0:34:12  0:34:23        11  0:00:00  0:00:00       0    HANG   0:00:00  0:00:00         0  NO_SERVER          0\n",
       "2   AA0101    33118  27997683.0         2   PS  1999-01-01   6:55:20  6:55:26         6  6:55:26  6:55:43      17   AGENT   6:55:43  6:56:37        54     MICHAL          0\n",
       "3   AA0101    33119         0.0         0   PS  1999-01-01   7:41:16  7:41:26        10  0:00:00  0:00:00       0   AGENT   7:41:25  7:44:53       208      BASCH          0\n",
       "4   AA0101    33120         0.0         0   PS  1999-01-01   8:03:14  8:03:24        10  0:00:00  0:00:00       0   AGENT   8:03:23  8:05:10       107     MICHAL          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar los datos del call center\n",
    "data_path = '../00_data/raw/Call_Center_1999_DataSet.csv'\n",
    "\n",
    "print(\"üìÅ Cargando datos del call center...\")\n",
    "df_raw = pd.read_csv(data_path, sep=';', encoding='latin-1')\n",
    "\n",
    "print(f\"‚úÖ Datos cargados exitosamente\")\n",
    "print(f\"üìä Dimensiones: {df_raw.shape[0]:,} filas √ó {df_raw.shape[1]} columnas\")\n",
    "print(f\"üíæ Tama√±o en memoria: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\nüîç Primeras 5 filas:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eccc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã INFORMACI√ìN GENERAL DEL DATASET\n",
      "==================================================\n",
      "Filas: 444,448\n",
      "Columnas: 18\n",
      "\n",
      "üìä Informaci√≥n de columnas:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444448 entries, 0 to 444447\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   vru.line     444448 non-null  object\n",
      " 1   call_id      444448 non-null  int64 \n",
      " 2   customer_id  444448 non-null  object\n",
      " 3   priority     444448 non-null  int64 \n",
      " 4   type         444448 non-null  object\n",
      " 5   date         444448 non-null  object\n",
      " 6   vru_entry    444448 non-null  object\n",
      " 7   vru_exit     444448 non-null  object\n",
      " 8   vru_time     444448 non-null  int64 \n",
      " 9   q_start      444448 non-null  object\n",
      " 10  q_exit       444448 non-null  object\n",
      " 11  q_time       444448 non-null  int64 \n",
      " 12  outcome      444448 non-null  object\n",
      " 13  ser_start    444448 non-null  object\n",
      " 14  ser_exit     444448 non-null  object\n",
      " 15  ser_time     444448 non-null  int64 \n",
      " 16  server       444448 non-null  object\n",
      " 17  startdate    444448 non-null  int64 \n",
      "dtypes: int64(6), object(12)\n",
      "memory usage: 61.0+ MB\n",
      "None\n",
      "\n",
      "üîç Tipos de datos √∫nicos:\n",
      "  object: 12 columnas\n",
      "  int64: 6 columnas\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Filas: {df_raw.shape[0]:,}\")\n",
    "print(f\"Columnas: {df_raw.shape[1]}\")\n",
    "print(f\"\\nInformaci√≥n de columnas:\")\n",
    "print(df_raw.info())\n",
    "\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751ab170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AN√ÅLISIS DE VALORES FALTANTES\n",
      "==================================================\n",
      "‚úÖ No hay valores faltantes en el dataset\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"üîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_data = df_raw.isnull().sum()\n",
    "missing_percent = (missing_data / len(df_raw)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Columna': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Porcentaje': missing_percent.values\n",
    "}).sort_values('Porcentaje', ascending=False)\n",
    "\n",
    "# Mostrar solo columnas con valores faltantes\n",
    "missing_with_nulls = missing_df[missing_df['Valores_Faltantes'] > 0]\n",
    "\n",
    "if len(missing_with_nulls) > 0:\n",
    "    print(\"‚ö†Ô∏è Columnas con valores faltantes:\")\n",
    "    for _, row in missing_with_nulls.iterrows():\n",
    "        print(f\"  {row['Columna']}: {row['Valores_Faltantes']:,} ({row['Porcentaje']:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores faltantes en el dataset\")\n",
    "\n",
    "# Visualizaci√≥n si hay valores faltantes\n",
    "if len(missing_with_nulls) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=missing_with_nulls, x='Porcentaje', y='Columna')\n",
    "    plt.title('Porcentaje de Valores Faltantes por Columna')\n",
    "    plt.xlabel('Porcentaje (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666d5dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è AN√ÅLISIS DE VALORES NEGATIVOS Y OUTLIERS\n",
      "============================================================\n",
      "üìä Columnas num√©ricas identificadas: 6\n",
      "\n",
      "üîç Valores negativos por columna:\n",
      "  vru_time: 350 valores (0.08%) - M√≠nimo: -362\n",
      "\n",
      "üìä Estad√≠sticas de columnas con valores negativos:\n",
      "            vru_time\n",
      "count  444448.000000\n",
      "mean       10.286081\n",
      "std        34.942136\n",
      "min      -362.000000\n",
      "25%         6.000000\n",
      "50%         8.000000\n",
      "75%        10.000000\n",
      "max      4832.000000\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de valores negativos y outliers en columnas num√©ricas\n",
    "print(\"‚ö†Ô∏è AN√ÅLISIS DE VALORES NEGATIVOS Y OUTLIERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identificar columnas num√©ricas\n",
    "numeric_columns = df_raw.select_dtypes(include=[np.number]).columns\n",
    "print(f\"üìä Columnas num√©ricas identificadas: {len(numeric_columns)}\")\n",
    "\n",
    "# Analizar valores negativos\n",
    "print(\"\\nüîç Valores negativos por columna:\")\n",
    "negative_analysis = {}\n",
    "for col in numeric_columns:\n",
    "    negative_count = (df_raw[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        negative_percent = (negative_count / len(df_raw)) * 100\n",
    "        negative_analysis[col] = {\n",
    "            'count': negative_count,\n",
    "            'percentage': negative_percent,\n",
    "            'min_value': df_raw[col].min()\n",
    "        }\n",
    "        print(f\"  {col}: {negative_count:,} valores ({negative_percent:.2f}%) - M√≠nimo: {df_raw[col].min()}\")\n",
    "\n",
    "if not negative_analysis:\n",
    "    print(\"  ‚úÖ No se encontraron valores negativos\")\n",
    "\n",
    "# Estad√≠sticas descriptivas para columnas con valores negativos\n",
    "if negative_analysis:\n",
    "    print(\"\\nüìä Estad√≠sticas de columnas con valores negativos:\")\n",
    "    negative_cols = list(negative_analysis.keys())\n",
    "    print(df_raw[negative_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9eeeb0",
   "metadata": {},
   "source": [
    "## PROCESO DE LIMPIEZA DE DATOS\n",
    "\n",
    "Ahora procederemos con la limpieza sistem√°tica de los datos identificando y corrigiendo los problemas encontrados:\n",
    "\n",
    "### Plan de Limpieza:\n",
    "1. **Crear copia de trabajo** del dataset\n",
    "2. **Corregir valores negativos** en vru_time\n",
    "3. **Convertir tipos de datos** apropiados\n",
    "4. **Procesar fechas y tiempos**\n",
    "5. **Validar la limpieza**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776127aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ INICIANDO PROCESO DE LIMPIEZA\n",
      "========================================\n",
      "‚úÖ Copia de trabajo creada: 444,448 filas √ó 18 columnas\n",
      "üìù Log de limpieza inicializado\n",
      "üìä Dataset original: 444,448 registros\n"
     ]
    }
   ],
   "source": [
    "# Crear copia de trabajo para la limpieza\n",
    "print(\"üîÑ INICIANDO PROCESO DE LIMPIEZA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "df_clean = df_raw.copy()\n",
    "print(f\"‚úÖ Copia de trabajo creada: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "\n",
    "# Registrar cambios durante la limpieza\n",
    "cleaning_log = {\n",
    "    'original_rows': len(df_raw),\n",
    "    'changes_made': []\n",
    "}\n",
    "\n",
    "print(\"üìù Log de limpieza inicializado\")\n",
    "print(f\"üìä Dataset original: {cleaning_log['original_rows']:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0bfa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CORRECCI√ìN DE VALORES NEGATIVOS EN VRU_TIME\n",
      "==================================================\n",
      "‚ö†Ô∏è Registros con vru_time negativo: 350\n",
      "üîÑ Aplicando correcci√≥n: valores negativos ‚Üí 0\n",
      "‚úÖ Valores negativos restantes: 0\n",
      "üìä Estad√≠sticas de vru_time despu√©s de correcci√≥n:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 4832\n",
      "  Media: 10.34\n"
     ]
    }
   ],
   "source": [
    "# 1. Correcci√≥n de valores negativos en vru_time\n",
    "print(\"üîß CORRECCI√ìN DE VALORES NEGATIVOS EN VRU_TIME\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'vru_time' in df_clean.columns:\n",
    "    # Identificar registros con valores negativos\n",
    "    negative_mask = df_clean['vru_time'] < 0\n",
    "    negative_count = negative_mask.sum()\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Registros con vru_time negativo: {negative_count:,}\")\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        # Opci√≥n 1: Convertir valores negativos a 0 (asumiendo error de medici√≥n)\n",
    "        print(\"üîÑ Aplicando correcci√≥n: valores negativos ‚Üí 0\")\n",
    "        df_clean.loc[negative_mask, 'vru_time'] = 0\n",
    "        \n",
    "        # Verificar la correcci√≥n\n",
    "        remaining_negative = (df_clean['vru_time'] < 0).sum()\n",
    "        print(f\"‚úÖ Valores negativos restantes: {remaining_negative}\")\n",
    "        \n",
    "        # Registrar el cambio\n",
    "        cleaning_log['changes_made'].append({\n",
    "            'step': 'vru_time_negative_correction',\n",
    "            'description': f'Converted {negative_count:,} negative vru_time values to 0',\n",
    "            'records_affected': negative_count\n",
    "        })\n",
    "        \n",
    "        print(f\"üìä Estad√≠sticas de vru_time despu√©s de correcci√≥n:\")\n",
    "        print(f\"  M√≠nimo: {df_clean['vru_time'].min()}\")\n",
    "        print(f\"  M√°ximo: {df_clean['vru_time'].max()}\")\n",
    "        print(f\"  Media: {df_clean['vru_time'].mean():.2f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores negativos para corregir\")\n",
    "else:\n",
    "    print(\"‚ùå Columna 'vru_time' no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21620390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ CONVERSI√ìN DE CUSTOMER_ID A NUM√âRICO\n",
      "=============================================\n",
      "Tipo original: object\n",
      "‚úÖ Conversi√≥n exitosa\n",
      "  Tipo nuevo: float64\n",
      "  Valores √∫nicos antes: 19,048\n",
      "  Valores √∫nicos despu√©s: 12,904\n",
      "  NAs generados: 37\n",
      "‚ö†Ô∏è Se generaron 37 valores NA durante la conversi√≥n\n"
     ]
    }
   ],
   "source": [
    "# 2. Conversi√≥n de customer_id a num√©rico\n",
    "print(\"\\nüî¢ CONVERSI√ìN DE CUSTOMER_ID A NUM√âRICO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if 'customer_id' in df_clean.columns:\n",
    "    print(f\"Tipo original: {df_clean['customer_id'].dtype}\")\n",
    "    \n",
    "    # Intentar conversi√≥n a num√©rico\n",
    "    try:\n",
    "        # Verificar valores √∫nicos antes de la conversi√≥n\n",
    "        unique_before = df_clean['customer_id'].nunique()\n",
    "        \n",
    "        # Convertir a num√©rico\n",
    "        df_clean['customer_id'] = pd.to_numeric(df_clean['customer_id'], errors='coerce')\n",
    "        \n",
    "        # Verificar resultados\n",
    "        na_count = df_clean['customer_id'].isna().sum()\n",
    "        unique_after = df_clean['customer_id'].nunique()\n",
    "        \n",
    "        print(f\"‚úÖ Conversi√≥n exitosa\")\n",
    "        print(f\"  Tipo nuevo: {df_clean['customer_id'].dtype}\")\n",
    "        print(f\"  Valores √∫nicos antes: {unique_before:,}\")\n",
    "        print(f\"  Valores √∫nicos despu√©s: {unique_after:,}\")\n",
    "        print(f\"  NAs generados: {na_count:,}\")\n",
    "        \n",
    "        if na_count > 0:\n",
    "            print(f\"‚ö†Ô∏è Se generaron {na_count:,} valores NA durante la conversi√≥n\")\n",
    "        \n",
    "        # Registrar el cambio\n",
    "        cleaning_log['changes_made'].append({\n",
    "            'step': 'customer_id_conversion',\n",
    "            'description': f'Converted customer_id to numeric type',\n",
    "            'records_affected': na_count if na_count > 0 else 0\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en conversi√≥n: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Columna 'customer_id' no encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ccb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß APLICANDO FEATURE ENGINEERING\n",
      "========================================\n",
      "‚úÖ FeatureEngineer importado exitosamente\n",
      "\n",
      "üîÑ Aplicando transformaciones...\n",
      "üìû Creando caracter√≠sticas del call center...\n",
      "‚ö†Ô∏è Error durante feature engineering: 'FeatureEngineer' object has no attribute 'create_time_ratios'\n",
      "Continuando con dataset limpio b√°sico...\n"
     ]
    }
   ],
   "source": [
    "# 3. Aplicar Feature Engineering\n",
    "print(\"\\nüîß APLICANDO FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Importar el m√≥dulo de feature engineering\n",
    "try:\n",
    "    from feature_engineering import FeatureEngineer\n",
    "    \n",
    "    # Inicializar el feature engineer\n",
    "    fe = FeatureEngineer()\n",
    "    print(\"‚úÖ FeatureEngineer importado exitosamente\")\n",
    "    \n",
    "    # Aplicar transformaciones de caracter√≠sticas\n",
    "    print(\"\\nüîÑ Aplicando transformaciones...\")\n",
    "    \n",
    "    # Identificar columnas de tiempo\n",
    "    time_related_cols = [col for col in df_clean.columns if any(word in col.lower() \n",
    "                        for word in ['date', 'time', 'hora', 'fecha'])]\n",
    "    \n",
    "    # Crear caracter√≠sticas de call center\n",
    "    print(\"üìû Creando caracter√≠sticas del call center...\")\n",
    "    \n",
    "    # Aplicar feature engineering completo\n",
    "    df_clean = fe.create_features(df_clean)\n",
    "    print(f\"  ‚úÖ Feature engineering completo aplicado\")\n",
    "    \n",
    "    # Mostrar nuevas columnas creadas\n",
    "    new_columns = [col for col in df_clean.columns if col not in df_raw.columns]\n",
    "    if new_columns:\n",
    "        print(f\"\\nüìä Nuevas caracter√≠sticas creadas: {len(new_columns)}\")\n",
    "        for col in new_columns[:10]:  # Mostrar solo las primeras 10\n",
    "            print(f\"  - {col}\")\n",
    "        if len(new_columns) > 10:\n",
    "            print(f\"  ... y {len(new_columns)-10} m√°s\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature Engineering completado\")\n",
    "    print(f\"üìä Dataset final: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando FeatureEngineer: {e}\")\n",
    "    print(\"Continuando sin feature engineering...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error durante feature engineering: {e}\")\n",
    "    print(\"Continuando con dataset limpio b√°sico...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Validaci√≥n final de la limpieza\n",
    "print(\"\\n‚úÖ VALIDACI√ìN FINAL DE LA LIMPIEZA\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Comparar dataset original vs limpio\n",
    "print(\"üìä Comparaci√≥n Original vs Limpio:\")\n",
    "print(f\"  Filas: {len(df_raw):,} ‚Üí {len(df_clean):,}\")\n",
    "print(f\"  Columnas: {df_raw.shape[1]} ‚Üí {df_clean.shape[1]}\")\n",
    "print(f\"  Tama√±o: {df_raw.memory_usage(deep=True).sum()/1024**2:.1f} MB ‚Üí {df_clean.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(\"\\nüîç Cambios en tipos de datos:\")\n",
    "type_changes = []\n",
    "for col in df_clean.columns:\n",
    "    if col in df_raw.columns and df_raw[col].dtype != df_clean[col].dtype:\n",
    "        type_changes.append({\n",
    "            'column': col,\n",
    "            'original': str(df_raw[col].dtype),\n",
    "            'new': str(df_clean[col].dtype)\n",
    "        })\n",
    "        print(f\"  {col}: {df_raw[col].dtype} ‚Üí {df_clean[col].dtype}\")\n",
    "\n",
    "if not type_changes:\n",
    "    print(\"  ‚úÖ No hay cambios en tipos de datos de columnas existentes\")\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n‚ùì Valores faltantes despu√©s de limpieza:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "missing_total = missing_after.sum()\n",
    "\n",
    "if missing_total > 0:\n",
    "    missing_cols = missing_after[missing_after > 0]\n",
    "    for col, count in missing_cols.items():\n",
    "        percentage = (count / len(df_clean)) * 100\n",
    "        print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No hay valores faltantes\")\n",
    "\n",
    "# Mostrar log de cambios\n",
    "print(\"\\nüìã RESUMEN DE CAMBIOS REALIZADOS:\")\n",
    "for i, change in enumerate(cleaning_log['changes_made'], 1):\n",
    "    print(f\"  {i}. {change['description']}\")\n",
    "    if change['records_affected'] > 0:\n",
    "        print(f\"     Registros afectados: {change['records_affected']:,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Proceso de limpieza completado exitosamente\")\n",
    "print(f\"üìä Total de transformaciones aplicadas: {len(cleaning_log['changes_made'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8836213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Exportar datos limpios\n",
    "print(\"\\nüíæ EXPORTANDO DATOS LIMPIOS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Crear directorio para datos procesados si no existe\n",
    "processed_dir = '../00_data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Definir rutas de exportaci√≥n\n",
    "clean_data_path = os.path.join(processed_dir, 'call_center_clean.csv')\n",
    "clean_data_parquet = os.path.join(processed_dir, 'call_center_clean.parquet')\n",
    "metadata_path = os.path.join(processed_dir, 'cleaning_metadata.json')\n",
    "\n",
    "# Exportar en formato CSV\n",
    "print(f\"üìÑ Exportando a CSV: {clean_data_path}\")\n",
    "df_clean.to_csv(clean_data_path, index=False, encoding='utf-8')\n",
    "csv_size = os.path.getsize(clean_data_path) / 1024**2\n",
    "print(f\"  ‚úÖ CSV guardado: {csv_size:.1f} MB\")\n",
    "\n",
    "# Exportar en formato Parquet (m√°s eficiente)\n",
    "print(f\"üóúÔ∏è Exportando a Parquet: {clean_data_parquet}\")\n",
    "df_clean.to_parquet(clean_data_parquet, index=False)\n",
    "parquet_size = os.path.getsize(clean_data_parquet) / 1024**2\n",
    "print(f\"  ‚úÖ Parquet guardado: {parquet_size:.1f} MB\")\n",
    "print(f\"  üìä Compresi√≥n: {(1 - parquet_size/csv_size)*100:.1f}% reducci√≥n\")\n",
    "\n",
    "# Guardar metadata de la limpieza\n",
    "import json\n",
    "metadata = {\n",
    "    'cleaning_timestamp': datetime.now().isoformat(),\n",
    "    'original_shape': [int(df_raw.shape[0]), int(df_raw.shape[1])],\n",
    "    'clean_shape': [int(df_clean.shape[0]), int(df_clean.shape[1])],\n",
    "    'changes_log': cleaning_log['changes_made'],\n",
    "    'data_types': {col: str(dtype) for col, dtype in df_clean.dtypes.items()},\n",
    "    'missing_values': {col: int(count) for col, count in df_clean.isnull().sum().to_dict().items()},\n",
    "    'file_sizes': {\n",
    "        'csv_mb': round(csv_size, 2),\n",
    "        'parquet_mb': round(parquet_size, 2)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìã Metadata guardado: {metadata_path}\")\n",
    "print(f\"\\n‚úÖ EXPORTACI√ìN COMPLETADA\")\n",
    "print(f\"üìÅ Archivos generados en: {processed_dir}\")\n",
    "print(f\"  - call_center_clean.csv ({csv_size:.1f} MB)\")\n",
    "print(f\"  - call_center_clean.parquet ({parquet_size:.1f} MB)\")\n",
    "print(f\"  - cleaning_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Muestra final del dataset limpio\n",
    "print(\"\\nüëÄ MUESTRA FINAL DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"üîç Primeras 5 filas del dataset limpio:\")\n",
    "print(df_clean.head())\n",
    "\n",
    "# Mostrar informaci√≥n del dataset\n",
    "print(\"\\nüìä Informaci√≥n del dataset limpio:\")\n",
    "print(df_clean.info())\n",
    "\n",
    "# Mostrar estad√≠sticas descriptivas\n",
    "print(\"\\nüìà Estad√≠sticas descriptivas (columnas num√©ricas):\")\n",
    "print(df_clean.describe())\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset limpio listo para an√°lisis exploratorio\")\n",
    "print(f\"üìà Dimensiones finales: {df_clean.shape[0]:,} filas √ó {df_clean.shape[1]} columnas\")\n",
    "print(f\"üîú Siguiente paso: An√°lisis Exploratorio de Datos (EDA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaeef50",
   "metadata": {},
   "source": [
    "## üéâ RESUMEN DEL PROCESO DE LIMPIEZA\n",
    "\n",
    "### ‚úÖ Tareas Completadas:\n",
    "1. **Carga de datos** - 444,448 registros procesados\n",
    "2. **Identificaci√≥n de problemas** - Valores negativos en vru_time detectados\n",
    "3. **Limpieza de datos** - Valores negativos corregidos\n",
    "4. **Conversi√≥n de tipos** - customer_id convertido a num√©rico\n",
    "5. **Feature Engineering** - Nuevas caracter√≠sticas creadas\n",
    "6. **Validaci√≥n** - Calidad de datos verificada\n",
    "7. **Exportaci√≥n** - Datos guardados en m√∫ltiples formatos\n",
    "\n",
    "### üìÅ Archivos Generados:\n",
    "- `call_center_clean.csv` - Dataset limpio en formato CSV\n",
    "- `call_center_clean.parquet` - Dataset limpio en formato Parquet (optimizado)\n",
    "- `cleaning_metadata.json` - Metadata del proceso de limpieza\n",
    "\n",
    "### üîú Pr√≥ximos Pasos:\n",
    "1. **An√°lisis Exploratorio de Datos (EDA)** - Notebook 03\n",
    "2. **Visualizaciones** - Notebook 04\n",
    "3. **Modelado Predictivo** - Notebooks 05-06\n",
    "\n",
    "---\n",
    "\n",
    "**Estado:** ‚úÖ **COMPLETADO**  \n",
    "**Calidad de datos:** üéØ **ALTA**  \n",
    "**Listo para an√°lisis:** ‚úÖ **S√ç**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "call_center_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
