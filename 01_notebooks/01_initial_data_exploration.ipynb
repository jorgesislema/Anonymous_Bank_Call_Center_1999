{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf06d51",
   "metadata": {},
   "source": [
    "#  An√°lisis Exploratorio Inicial - Call Center 1999\n",
    "\n",
    "##  Anonymous Bank - An√°lisis de Datos de Call Center\n",
    "\n",
    "### Descripci√≥n del Proyecto\n",
    "\n",
    "Este notebook realiza la exploraci√≥n inicial del dataset del Call Center del \"Anonymous Bank\" correspondiente al a√±o 1999. El an√°lisis incluye:\n",
    "\n",
    "- **Carga y estructura de datos**: Comprensi√≥n del formato y dimensiones\n",
    "- **An√°lisis descriptivo**: Estad√≠sticas b√°sicas y distribuciones\n",
    "- **Calidad de datos**: Identificaci√≥n de valores faltantes y outliers\n",
    "- **An√°lisis temporal**: Patrones por fecha, hora y d√≠a de la semana\n",
    "- **An√°lisis operacional**: Tiempos de servicio, colas y eficiencia\n",
    "- **Visualizaciones iniciales**: Gr√°ficos exploratorios para entender los datos\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. **Comprender la estructura** del dataset y sus variables\n",
    "2. **Identificar patrones temporales** en las llamadas\n",
    "3. **Analizar la eficiencia operacional** del call center\n",
    "4. **Detectar anomal√≠as** y problemas de calidad de datos\n",
    "5. **Generar insights iniciales** para an√°lisis posteriores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc554c0",
   "metadata": {},
   "source": [
    "## 1. Impotacion de librer√≠as y datos necesarios.\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el an√°lisis exploratorio de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c68099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n",
      "üìä Pandas version: 2.2.3\n",
      "üî¢ NumPy version: 2.2.6\n",
      "üìà Matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "# Librer√≠as para manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configuraci√≥n de Plotly\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"üìà Matplotlib backend: {plt.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45d273",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. üìÇ Carga y Estructura de Datos\n",
    "\n",
    "### Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffac4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo encontrado: ../00_data/raw/Call_Center_1999_DataSet.csv\n",
      "üìè Tama√±o del archivo: 56.99 MB\n",
      "\n",
      "üì• Cargando datos...\n",
      "‚úÖ Datos cargados exitosamente en 1.13 segundos\n",
      "üìä Dimensiones del dataset: 444,448 filas x 18 columnas\n"
     ]
    }
   ],
   "source": [
    "# Definir ruta del archivo\n",
    "data_path = \"../00_data/raw/Call_Center_1999_DataSet.csv\"\n",
    "\n",
    "# Verificar que el archivo existe\n",
    "import os\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"‚úÖ Archivo encontrado: {data_path}\")\n",
    "    file_size = os.path.getsize(data_path) / (1024 * 1024)  # MB\n",
    "    print(f\"üìè Tama√±o del archivo: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Archivo no encontrado: {data_path}\")\n",
    "    \n",
    "# Cargar datos\n",
    "print(\"\\nüì• Cargando datos...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Cargar el dataset con par√°metros espec√≠ficos para este archivo\n",
    "    # El archivo usa punto y coma como separador y puede tener problemas de formato\n",
    "    df = pd.read_csv(\n",
    "        data_path, \n",
    "        sep=';',  # Usar punto y coma como separador\n",
    "        encoding='utf-8',  # Especificar encoding\n",
    "        on_bad_lines='skip',  # Omitir l√≠neas con problemas\n",
    "        quotechar='\"',  # Car√°cter de citas\n",
    "        low_memory=False  # Cargar todo en memoria para mejor rendimiento\n",
    "    )\n",
    "    \n",
    "    load_time = datetime.now() - start_time\n",
    "    print(f\"‚úÖ Datos cargados exitosamente en {load_time.total_seconds():.2f} segundos\")\n",
    "    print(f\"üìä Dimensiones del dataset: {df.shape[0]:,} filas x {df.shape[1]} columnas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar los datos: {e}\")\n",
    "    print(\"\\nüîß Intentando con diferentes par√°metros...\")\n",
    "    \n",
    "    try:\n",
    "        # Intento alternativo con diferentes par√°metros\n",
    "        df = pd.read_csv(\n",
    "            data_path,\n",
    "            sep=';',\n",
    "            encoding='latin-1',  # Encoding alternativo\n",
    "            on_bad_lines='skip',\n",
    "            quoting=1  # QUOTE_ALL\n",
    "        )\n",
    "        print(f\"‚úÖ Datos cargados con par√°metros alternativos\")\n",
    "        print(f\"üìä Dimensiones del dataset: {df.shape[0]:,} filas x {df.shape[1]} columnas\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Error persistente: {e2}\")\n",
    "        print(\"üí° Sugerencia: Verificar manualmente el formato del archivo CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ea418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INFORMACI√ìN GENERAL DEL DATASET\n",
      "==================================================\n",
      "N√∫mero de filas: 444,448\n",
      "N√∫mero de columnas: 18\n",
      "Memoria utilizada: 303.32 MB\n",
      "\n",
      "üìã COLUMNAS DEL DATASET:\n",
      "------------------------------\n",
      " 1. vru.line\n",
      " 2. call_id\n",
      " 3. customer_id\n",
      " 4. priority\n",
      " 5. type\n",
      " 6. date\n",
      " 7. vru_entry\n",
      " 8. vru_exit\n",
      " 9. vru_time\n",
      "10. q_start\n",
      "11. q_exit\n",
      "12. q_time\n",
      "13. outcome\n",
      "14. ser_start\n",
      "15. ser_exit\n",
      "16. ser_time\n",
      "17. server\n",
      "18. startdate\n",
      "\n",
      "üëÄ PRIMEROS 5 REGISTROS:\n",
      "==================================================\n",
      "Memoria utilizada: 303.32 MB\n",
      "\n",
      "üìã COLUMNAS DEL DATASET:\n",
      "------------------------------\n",
      " 1. vru.line\n",
      " 2. call_id\n",
      " 3. customer_id\n",
      " 4. priority\n",
      " 5. type\n",
      " 6. date\n",
      " 7. vru_entry\n",
      " 8. vru_exit\n",
      " 9. vru_time\n",
      "10. q_start\n",
      "11. q_exit\n",
      "12. q_time\n",
      "13. outcome\n",
      "14. ser_start\n",
      "15. ser_exit\n",
      "16. ser_time\n",
      "17. server\n",
      "18. startdate\n",
      "\n",
      "üëÄ PRIMEROS 5 REGISTROS:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vru.line</th>\n",
       "      <th>call_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>priority</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>vru_entry</th>\n",
       "      <th>vru_exit</th>\n",
       "      <th>vru_time</th>\n",
       "      <th>q_start</th>\n",
       "      <th>q_exit</th>\n",
       "      <th>q_time</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ser_start</th>\n",
       "      <th>ser_exit</th>\n",
       "      <th>ser_time</th>\n",
       "      <th>server</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33116</td>\n",
       "      <td>9664491</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:00:31</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>5</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:03:09</td>\n",
       "      <td>153</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>0:34:12</td>\n",
       "      <td>0:34:23</td>\n",
       "      <td>11</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>HANG</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_SERVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33118</td>\n",
       "      <td>27997683</td>\n",
       "      <td>2</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>6:55:20</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6</td>\n",
       "      <td>6:55:26</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>17</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>6:55:43</td>\n",
       "      <td>6:56:37</td>\n",
       "      <td>54</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>7:41:16</td>\n",
       "      <td>7:41:26</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>7:41:25</td>\n",
       "      <td>7:44:53</td>\n",
       "      <td>208</td>\n",
       "      <td>BASCH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA0101</td>\n",
       "      <td>33120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>8:03:14</td>\n",
       "      <td>8:03:24</td>\n",
       "      <td>10</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>8:03:23</td>\n",
       "      <td>8:05:10</td>\n",
       "      <td>107</td>\n",
       "      <td>MICHAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vru.line  call_id customer_id  priority type        date vru_entry vru_exit  \\\n",
       "0   AA0101    33116     9664491         2   PS  1999-01-01   0:00:31  0:00:36   \n",
       "1   AA0101    33117           0         0   PS  1999-01-01   0:34:12  0:34:23   \n",
       "2   AA0101    33118    27997683         2   PS  1999-01-01   6:55:20  6:55:26   \n",
       "3   AA0101    33119           0         0   PS  1999-01-01   7:41:16  7:41:26   \n",
       "4   AA0101    33120           0         0   PS  1999-01-01   8:03:14  8:03:24   \n",
       "\n",
       "   vru_time  q_start   q_exit  q_time outcome ser_start ser_exit  ser_time  \\\n",
       "0         5  0:00:36  0:03:09     153    HANG   0:00:00  0:00:00         0   \n",
       "1        11  0:00:00  0:00:00       0    HANG   0:00:00  0:00:00         0   \n",
       "2         6  6:55:26  6:55:43      17   AGENT   6:55:43  6:56:37        54   \n",
       "3        10  0:00:00  0:00:00       0   AGENT   7:41:25  7:44:53       208   \n",
       "4        10  0:00:00  0:00:00       0   AGENT   8:03:23  8:05:10       107   \n",
       "\n",
       "      server  startdate  \n",
       "0  NO_SERVER          0  \n",
       "1  NO_SERVER          0  \n",
       "2     MICHAL          0  \n",
       "3      BASCH          0  \n",
       "4     MICHAL          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspecci√≥n inicial de la estructura\n",
    "print(\"üîç INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"N√∫mero de filas: {df.shape[0]:,}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")\n",
    "print(f\"Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nüìã COLUMNAS DEL DATASET:\")\n",
    "print(\"-\" * 30)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Primeros registros\n",
    "print(\"\\nüëÄ PRIMEROS 5 REGISTROS:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6808c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TIPOS DE DATOS Y INFORMACI√ìN DETALLADA\n",
      "==================================================\n",
      "\n",
      "üìä Informaci√≥n del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444448 entries, 0 to 444447\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   vru.line     444448 non-null  object\n",
      " 1   call_id      444448 non-null  int64 \n",
      " 2   customer_id  444448 non-null  object\n",
      " 3   priority     444448 non-null  int64 \n",
      " 4   type         444448 non-null  object\n",
      " 5   date         444448 non-null  object\n",
      " 6   vru_entry    444448 non-null  object\n",
      " 7   vru_exit     444448 non-null  object\n",
      " 8   vru_time     444448 non-null  int64 \n",
      " 9   q_start      444448 non-null  object\n",
      " 10  q_exit       444448 non-null  object\n",
      " 11  q_time       444448 non-null  int64 \n",
      " 12  outcome      444448 non-null  object\n",
      " 13  ser_start    444448 non-null  object\n",
      " 14  ser_exit     444448 non-null  object\n",
      " 15  ser_time     444448 non-null  int64 \n",
      " 16  server       444448 non-null  object\n",
      " 17  startdate    444448 non-null  int64 \n",
      "dtypes: int64(6), object(12)\n",
      "memory usage: 303.3 MB\n",
      "\n",
      "üîç AN√ÅLISIS DE TIPOS DE DATOS:\n",
      "----------------------------------------\n",
      "\n",
      "Distribuci√≥n de tipos de datos:\n",
      "  object: 12 columnas\n",
      "  int64: 6 columnas\n",
      "\n",
      "üìä Columnas num√©ricas: 6\n",
      "üìù Columnas categ√≥ricas: 12\n",
      "üìÖ Columnas de fecha/hora: 0\n",
      "\n",
      "Columnas num√©ricas: ['call_id', 'priority', 'vru_time', 'q_time', 'ser_time', 'startdate']\n",
      "\n",
      "Columnas categ√≥ricas: ['vru.line', 'customer_id', 'type', 'date', 'vru_entry', 'vru_exit', 'q_start', 'q_exit', 'outcome', 'ser_start', 'ser_exit', 'server']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444448 entries, 0 to 444447\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   vru.line     444448 non-null  object\n",
      " 1   call_id      444448 non-null  int64 \n",
      " 2   customer_id  444448 non-null  object\n",
      " 3   priority     444448 non-null  int64 \n",
      " 4   type         444448 non-null  object\n",
      " 5   date         444448 non-null  object\n",
      " 6   vru_entry    444448 non-null  object\n",
      " 7   vru_exit     444448 non-null  object\n",
      " 8   vru_time     444448 non-null  int64 \n",
      " 9   q_start      444448 non-null  object\n",
      " 10  q_exit       444448 non-null  object\n",
      " 11  q_time       444448 non-null  int64 \n",
      " 12  outcome      444448 non-null  object\n",
      " 13  ser_start    444448 non-null  object\n",
      " 14  ser_exit     444448 non-null  object\n",
      " 15  ser_time     444448 non-null  int64 \n",
      " 16  server       444448 non-null  object\n",
      " 17  startdate    444448 non-null  int64 \n",
      "dtypes: int64(6), object(12)\n",
      "memory usage: 303.3 MB\n",
      "\n",
      "üîç AN√ÅLISIS DE TIPOS DE DATOS:\n",
      "----------------------------------------\n",
      "\n",
      "Distribuci√≥n de tipos de datos:\n",
      "  object: 12 columnas\n",
      "  int64: 6 columnas\n",
      "\n",
      "üìä Columnas num√©ricas: 6\n",
      "üìù Columnas categ√≥ricas: 12\n",
      "üìÖ Columnas de fecha/hora: 0\n",
      "\n",
      "Columnas num√©ricas: ['call_id', 'priority', 'vru_time', 'q_time', 'ser_time', 'startdate']\n",
      "\n",
      "Columnas categ√≥ricas: ['vru.line', 'customer_id', 'type', 'date', 'vru_entry', 'vru_exit', 'q_start', 'q_exit', 'outcome', 'ser_start', 'ser_exit', 'server']\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de tipos de datos\n",
    "print(\"üß™ TIPOS DE DATOS Y INFORMACI√ìN DETALLADA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Informaci√≥n b√°sica\n",
    "print(\"\\nüìä Informaci√≥n del DataFrame:\")\n",
    "df.info(memory_usage='deep')\n",
    "\n",
    "# Descripci√≥n de tipos de datos\n",
    "print(\"\\nüîç AN√ÅLISIS DE TIPOS DE DATOS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "data_types_summary = df.dtypes.value_counts()\n",
    "print(\"\\nDistribuci√≥n de tipos de datos:\")\n",
    "for dtype, count in data_types_summary.items():\n",
    "    print(f\"  {dtype}: {count} columnas\")\n",
    "\n",
    "# Identificar columnas categ√≥ricas vs num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìä Columnas num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"üìù Columnas categ√≥ricas: {len(categorical_cols)}\")\n",
    "print(f\"üìÖ Columnas de fecha/hora: {len(datetime_cols)}\")\n",
    "\n",
    "if numeric_cols:\n",
    "    print(f\"\\nColumnas num√©ricas: {numeric_cols}\")\n",
    "if categorical_cols:\n",
    "    print(f\"\\nColumnas categ√≥ricas: {categorical_cols}\")\n",
    "if datetime_cols:\n",
    "    print(f\"\\nColumnas fecha/hora: {datetime_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c32b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üìä An√°lisis Descriptivo B√°sico\n",
    "\n",
    "### Estad√≠sticas Descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a0dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_id</th>\n",
       "      <th>priority</th>\n",
       "      <th>vru_time</th>\n",
       "      <th>q_time</th>\n",
       "      <th>ser_time</th>\n",
       "      <th>startdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>444448.000000</td>\n",
       "      <td>444448.000000</td>\n",
       "      <td>444448.000000</td>\n",
       "      <td>444448.000000</td>\n",
       "      <td>444448.000000</td>\n",
       "      <td>444448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31928.737202</td>\n",
       "      <td>0.780143</td>\n",
       "      <td>10.286081</td>\n",
       "      <td>59.004304</td>\n",
       "      <td>152.561776</td>\n",
       "      <td>172.333974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13945.516813</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>34.942136</td>\n",
       "      <td>119.470328</td>\n",
       "      <td>282.372761</td>\n",
       "      <td>104.559247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1169.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-362.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21449.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35256.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42803.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55656.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4832.000000</td>\n",
       "      <td>28693.000000</td>\n",
       "      <td>61437.000000</td>\n",
       "      <td>334.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             call_id       priority       vru_time         q_time  \\\n",
       "count  444448.000000  444448.000000  444448.000000  444448.000000   \n",
       "mean    31928.737202       0.780143      10.286081      59.004304   \n",
       "std     13945.516813       0.888851      34.942136     119.470328   \n",
       "min      1169.000000       0.000000    -362.000000       0.000000   \n",
       "25%     21449.000000       0.000000       6.000000       0.000000   \n",
       "50%     35256.000000       0.000000       8.000000      16.000000   \n",
       "75%     42803.000000       2.000000      10.000000      79.000000   \n",
       "max     55656.000000       2.000000    4832.000000   28693.000000   \n",
       "\n",
       "            ser_time      startdate  \n",
       "count  444448.000000  444448.000000  \n",
       "mean      152.561776     172.333974  \n",
       "std       282.372761     104.559247  \n",
       "min         0.000000       0.000000  \n",
       "25%        12.000000      90.000000  \n",
       "50%        84.000000     181.000000  \n",
       "75%       185.000000     273.000000  \n",
       "max     61437.000000     334.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà AN√ÅLISIS DE DISTRIBUCIONES:\n",
      "----------------------------------------\n",
      "\n",
      "call_id:\n",
      "  M√≠nimo: 1169\n",
      "  M√°ximo: 55656\n",
      "  Media: 31928.74\n",
      "  Mediana: 35256.00\n",
      "  Desv. Est√°ndar: 13945.52\n",
      "  Outliers detectados: 0 (0.00%)\n",
      "\n",
      "priority:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 2\n",
      "  Media: 0.78\n",
      "  Mediana: 0.00\n",
      "  Desv. Est√°ndar: 0.89\n",
      "  Outliers detectados: 0 (0.00%)\n",
      "\n",
      "vru_time:\n",
      "  M√≠nimo: -362\n",
      "  M√°ximo: 4832\n",
      "  Media: 10.29\n",
      "  Mediana: 8.00\n",
      "  Desv. Est√°ndar: 34.94\n",
      "  Outliers detectados: 26683 (6.00%)\n",
      "\n",
      "q_time:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 28693\n",
      "  Media: 59.00\n",
      "  Mediana: 16.00\n",
      "  Desv. Est√°ndar: 119.47\n",
      "  Outliers detectados: 37237 (8.38%)\n",
      "\n",
      "ser_time:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 61437\n",
      "  Media: 152.56\n",
      "  Mediana: 84.00\n",
      "  Desv. Est√°ndar: 282.37\n",
      "  Outliers detectados: 32381 (7.29%)\n",
      "\n",
      "startdate:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 334\n",
      "  Media: 172.33\n",
      "  Mediana: 181.00\n",
      "  Desv. Est√°ndar: 104.56\n",
      "  Outliers detectados: 0 (0.00%)\n",
      "  Outliers detectados: 32381 (7.29%)\n",
      "\n",
      "startdate:\n",
      "  M√≠nimo: 0\n",
      "  M√°ximo: 334\n",
      "  Media: 172.33\n",
      "  Mediana: 181.00\n",
      "  Desv. Est√°ndar: 104.56\n",
      "  Outliers detectados: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Estad√≠sticas descriptivas para variables num√©ricas\n",
    "if numeric_cols:\n",
    "    print(\"üî¢ ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    desc_stats = df[numeric_cols].describe()\n",
    "    display(desc_stats)\n",
    "    \n",
    "    # An√°lisis adicional de distribuciones\n",
    "    print(\"\\nüìà AN√ÅLISIS DE DISTRIBUCIONES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  M√≠nimo: {df[col].min()}\")\n",
    "            print(f\"  M√°ximo: {df[col].max()}\")\n",
    "            print(f\"  Media: {df[col].mean():.2f}\")\n",
    "            print(f\"  Mediana: {df[col].median():.2f}\")\n",
    "            print(f\"  Desv. Est√°ndar: {df[col].std():.2f}\")\n",
    "            \n",
    "            # Detectar outliers usando IQR\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            print(f\"  Outliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron columnas num√©ricas en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de variables categ√≥ricas\n",
    "if categorical_cols:\n",
    "    print(\"\\nüìù ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES CATEG√ìRICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nüè∑Ô∏è {col.upper()}:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Valores √∫nicos\n",
    "        unique_values = df[col].nunique()\n",
    "        print(f\"Valores √∫nicos: {unique_values}\")\n",
    "        \n",
    "        # Distribuci√≥n de frecuencias\n",
    "        value_counts = df[col].value_counts()\n",
    "        print(f\"\\nTop 10 valores m√°s frecuentes:\")\n",
    "        for i, (value, count) in enumerate(value_counts.head(10).items(), 1):\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {i:2d}. {value}: {count:,} ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Verificar si hay valores nulos representados como strings\n",
    "        null_like_values = ['null', 'NULL', 'nan', 'NaN', 'none', 'None', '', ' ']\n",
    "        null_like_count = df[col].isin(null_like_values).sum()\n",
    "        if null_like_count > 0:\n",
    "            print(f\"‚ö†Ô∏è Valores nulos impl√≠citos: {null_like_count}\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron columnas categ√≥ricas en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9f198",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üßπ An√°lisis de Calidad de Datos\n",
    "\n",
    "### Valores Faltantes y Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"üîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Contar valores nulos por columna\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "# Crear DataFrame resumen\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Columna': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Porcentaje': missing_percentage.values\n",
    "}).sort_values('Valores_Faltantes', ascending=False)\n",
    "\n",
    "# Mostrar solo columnas con valores faltantes\n",
    "missing_summary_filtered = missing_summary[missing_summary['Valores_Faltantes'] > 0]\n",
    "\n",
    "if len(missing_summary_filtered) > 0:\n",
    "    print(\"\\nüìã COLUMNAS CON VALORES FALTANTES:\")\n",
    "    print(\"-\" * 50)\n",
    "    display(missing_summary_filtered)\n",
    "    \n",
    "    print(f\"\\nüìä RESUMEN:\")\n",
    "    print(f\"Total de columnas con valores faltantes: {len(missing_summary_filtered)}\")\n",
    "    print(f\"Columna con m√°s valores faltantes: {missing_summary_filtered.iloc[0]['Columna']} ({missing_summary_filtered.iloc[0]['Porcentaje']:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ ¬°Excelente! No hay valores faltantes en el dataset\")\n",
    "\n",
    "# An√°lisis de patrones de valores faltantes\n",
    "print(\"\\nüîó PATRONES DE VALORES FALTANTES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percentage_total = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"Total de celdas: {total_cells:,}\")\n",
    "print(f\"Total de valores faltantes: {total_missing:,}\")\n",
    "print(f\"Porcentaje total de valores faltantes: {missing_percentage_total:.4f}%\")\n",
    "\n",
    "# Filas completamente llenas\n",
    "complete_rows = df.dropna().shape[0]\n",
    "print(f\"Filas completamente llenas: {complete_rows:,} ({complete_rows/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de valores faltantes\n",
    "if missing_summary_filtered.shape[0] > 0:\n",
    "    # Crear visualizaci√≥n con plotly\n",
    "    fig = px.bar(\n",
    "        missing_summary_filtered.head(10), \n",
    "        x='Columna', \n",
    "        y='Porcentaje',\n",
    "        title='Top 10 Columnas con Valores Faltantes',\n",
    "        labels={'Porcentaje': 'Porcentaje de Valores Faltantes (%)', 'Columna': 'Columnas'},\n",
    "        color='Porcentaje',\n",
    "        color_continuous_scale='Reds'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        xaxis_tickangle=-45,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Heatmap de valores faltantes (si hay columnas con missing values)\n",
    "    if len(missing_summary_filtered) <= 20:  # Solo para datasets manejables\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Seleccionar solo columnas con valores faltantes para el heatmap\n",
    "        cols_with_missing = missing_summary_filtered['Columna'].tolist()\n",
    "        \n",
    "        if len(cols_with_missing) > 0:\n",
    "            # Crear matriz de valores faltantes\n",
    "            missing_matrix = df[cols_with_missing].isnull()\n",
    "            \n",
    "            sns.heatmap(\n",
    "                missing_matrix.head(1000),  # Mostrar solo primeras 1000 filas\n",
    "                cbar=True,\n",
    "                cmap='viridis',\n",
    "                yticklabels=False\n",
    "            )\n",
    "            \n",
    "            plt.title('Patr√≥n de Valores Faltantes (Primeras 1000 filas)')\n",
    "            plt.xlabel('Columnas')\n",
    "            plt.ylabel('Filas')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ No hay valores faltantes para visualizar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce4634",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. üìû An√°lisis Espec√≠fico del Call Center\n",
    "\n",
    "### Preparaci√≥n de Datos de Tiempo\n",
    "\n",
    "Analizaremos los campos espec√≠ficos del call center como tiempos de VRU, cola y servicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar campos relacionados con tiempo y call center\n",
    "print(\"üïê IDENTIFICACI√ìN DE CAMPOS DE TIEMPO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Buscar columnas que probablemente contengan informaci√≥n de tiempo\n",
    "time_related_cols = []\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(keyword in col_lower for keyword in ['time', 'hora', 'fecha', 'date', 'vru', 'queue', 'start', 'exit', 'entry']):\n",
    "        time_related_cols.append(col)\n",
    "\n",
    "print(f\"\\nüìã Columnas relacionadas con tiempo identificadas: {len(time_related_cols)}\")\n",
    "for i, col in enumerate(time_related_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Mostrar sample de estas columnas\n",
    "if time_related_cols:\n",
    "    print(\"\\nüëÄ MUESTRA DE DATOS DE TIEMPO:\")\n",
    "    print(\"-\" * 50)\n",
    "    sample_time_data = df[time_related_cols].head(10)\n",
    "    display(sample_time_data)\n",
    "    \n",
    "    # An√°lisis de formato de tiempo\n",
    "    print(\"\\nüîç AN√ÅLISIS DE FORMATO:\")\n",
    "    print(\"-\" * 30)\n",
    "    for col in time_related_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Tipo de dato: {df[col].dtype}\")\n",
    "        print(f\"  Valores √∫nicos: {df[col].nunique():,}\")\n",
    "        print(f\"  Valores no nulos: {df[col].notna().sum():,}\")\n",
    "        \n",
    "        # Mostrar algunos ejemplos de valores\n",
    "        non_null_values = df[col].dropna().head(5).tolist()\n",
    "        print(f\"  Ejemplos: {non_null_values}\")\n",
    "else:\n",
    "    print(\"‚ùå No se identificaron columnas de tiempo claras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de campos espec√≠ficos del call center\n",
    "print(\"\\nüìä AN√ÅLISIS DE CAMPOS ESPEC√çFICOS DEL CALL CENTER\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Buscar campos t√≠picos de call center\n",
    "call_center_fields = {\n",
    "    'outcome': 'Resultado de la llamada',\n",
    "    'type': 'Tipo de servicio',\n",
    "    'priority': 'Prioridad del cliente',\n",
    "    'server': 'Agente que atendi√≥',\n",
    "    'customer_id': 'ID del cliente',\n",
    "    'call_id': 'ID de la llamada'\n",
    "}\n",
    "\n",
    "# Verificar qu√© campos existen en el dataset\n",
    "existing_fields = {}\n",
    "for field, description in call_center_fields.items():\n",
    "    matching_cols = [col for col in df.columns if field.lower() in col.lower()]\n",
    "    if matching_cols:\n",
    "        existing_fields[matching_cols[0]] = description\n",
    "\n",
    "if existing_fields:\n",
    "    print(\"\\n‚úÖ CAMPOS DE CALL CENTER ENCONTRADOS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for field, description in existing_fields.items():\n",
    "        print(f\"\\nüè∑Ô∏è {field.upper()} ({description}):\")\n",
    "        print(f\"   Tipo: {df[field].dtype}\")\n",
    "        print(f\"   Valores √∫nicos: {df[field].nunique():,}\")\n",
    "        print(f\"   Valores no nulos: {df[field].notna().sum():,}\")\n",
    "        \n",
    "        # Para campos categ√≥ricos, mostrar distribuci√≥n\n",
    "        if df[field].dtype == 'object' or df[field].nunique() < 50:\n",
    "            value_counts = df[field].value_counts().head(5)\n",
    "            print(f\"   Top 5 valores:\")\n",
    "            for value, count in value_counts.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"     {value}: {count:,} ({percentage:.2f}%)\")\n",
    "        else:\n",
    "            # Para campos num√©ricos, mostrar estad√≠sticas b√°sicas\n",
    "            print(f\"   Min: {df[field].min()}, Max: {df[field].max()}\")\n",
    "            print(f\"   Media: {df[field].mean():.2f}, Mediana: {df[field].median():.2f}\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontraron campos t√≠picos de call center con nombres est√°ndar\")\n",
    "    print(\"üí° Revisaremos todas las columnas para identificar patrones...\")\n",
    "    \n",
    "    # Mostrar todas las columnas para an√°lisis manual\n",
    "    print(\"\\nüìã TODAS LAS COLUMNAS DISPONIBLES:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col} ({df[col].dtype}, {df[col].nunique()} √∫nicos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60ac78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. üìÖ An√°lisis Temporal\n",
    "\n",
    "### Patrones de Llamadas por Tiempo\n",
    "\n",
    "Analizaremos los patrones temporales de las llamadas del call center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de campos de fecha y hora\n",
    "print(\"üìÖ PROCESAMIENTO DE CAMPOS TEMPORALES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Buscar columnas que parezcan fechas\n",
    "date_cols = []\n",
    "for col in df.columns:\n",
    "    # Verificar si la columna contiene fechas\n",
    "    if df[col].dtype == 'object':\n",
    "        # Tomar una muestra para verificar formato de fecha\n",
    "        sample_values = df[col].dropna().head(100)\n",
    "        \n",
    "        # Intentar parsear como fecha\n",
    "        try:\n",
    "            # Verificar diferentes formatos comunes\n",
    "            date_formats = ['%Y%m%d', '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y']\n",
    "            \n",
    "            for date_format in date_formats:\n",
    "                try:\n",
    "                    parsed_dates = pd.to_datetime(sample_values, format=date_format, errors='coerce')\n",
    "                    if parsed_dates.notna().sum() > len(sample_values) * 0.8:  # 80% de √©xito\n",
    "                        date_cols.append((col, date_format))\n",
    "                        print(f\"‚úÖ Campo de fecha detectado: {col} (formato: {date_format})\")\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Buscar columnas que parezcan tiempo\n",
    "time_cols = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        sample_values = df[col].dropna().head(100)\n",
    "        \n",
    "        # Verificar formato de tiempo (HHMMSS, HH:MM:SS, etc.)\n",
    "        time_patterns = [r'^\\d{6}$', r'^\\d{2}:\\d{2}:\\d{2}$', r'^\\d{1,2}:\\d{2}$']\n",
    "        \n",
    "        for pattern in time_patterns:\n",
    "            import re\n",
    "            matches = sample_values.astype(str).str.match(pattern).sum()\n",
    "            if matches > len(sample_values) * 0.8:\n",
    "                time_cols.append(col)\n",
    "                print(f\"‚úÖ Campo de tiempo detectado: {col}\")\n",
    "                break\n",
    "\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"Campos de fecha detectados: {len(date_cols)}\")\n",
    "print(f\"Campos de tiempo detectados: {len(time_cols)}\")\n",
    "\n",
    "if date_cols:\n",
    "    print(\"\\nüìÖ Campos de fecha:\")\n",
    "    for col, format_str in date_cols:\n",
    "        print(f\"  - {col} (formato: {format_str})\")\n",
    "        \n",
    "if time_cols:\n",
    "    print(\"\\nüïê Campos de tiempo:\")\n",
    "    for col in time_cols:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641da2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversi√≥n de campos de fecha y tiempo\n",
    "if date_cols:\n",
    "    print(\"\\nüîÑ CONVERSI√ìN DE CAMPOS TEMPORALES\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Convertir campos de fecha\n",
    "    for col, date_format in date_cols:\n",
    "        try:\n",
    "            # Crear nueva columna con fecha convertida\n",
    "            new_col_name = f\"{col}_parsed\"\n",
    "            df[new_col_name] = pd.to_datetime(df[col], format=date_format, errors='coerce')\n",
    "            \n",
    "            # Verificar conversi√≥n\n",
    "            conversion_success = df[new_col_name].notna().sum()\n",
    "            total_non_null = df[col].notna().sum()\n",
    "            success_rate = (conversion_success / total_non_null) * 100 if total_non_null > 0 else 0\n",
    "            \n",
    "            print(f\"‚úÖ {col} ‚Üí {new_col_name}\")\n",
    "            print(f\"   Conversiones exitosas: {conversion_success:,}/{total_non_null:,} ({success_rate:.1f}%)\")\n",
    "            \n",
    "            if conversion_success > 0:\n",
    "                # Extraer componentes de fecha\n",
    "                df[f\"{col}_year\"] = df[new_col_name].dt.year\n",
    "                df[f\"{col}_month\"] = df[new_col_name].dt.month\n",
    "                df[f\"{col}_day\"] = df[new_col_name].dt.day\n",
    "                df[f\"{col}_weekday\"] = df[new_col_name].dt.day_name()\n",
    "                df[f\"{col}_quarter\"] = df[new_col_name].dt.quarter\n",
    "                \n",
    "                # Mostrar rango de fechas\n",
    "                min_date = df[new_col_name].min()\n",
    "                max_date = df[new_col_name].max()\n",
    "                print(f\"   Rango: {min_date.strftime('%Y-%m-%d')} a {max_date.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "                # Calcular duraci√≥n del per√≠odo\n",
    "                duration = max_date - min_date\n",
    "                print(f\"   Duraci√≥n: {duration.days} d√≠as\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error convirtiendo {col}: {e}\")\n",
    "\n",
    "# An√°lisis temporal b√°sico\n",
    "if date_cols and len([col for col, _ in date_cols if f\"{col}_parsed\" in df.columns]) > 0:\n",
    "    print(\"\\nüìä AN√ÅLISIS TEMPORAL B√ÅSICO\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Usar la primera fecha v√°lida para an√°lisis\n",
    "    date_col = None\n",
    "    for col, _ in date_cols:\n",
    "        if f\"{col}_parsed\" in df.columns and df[f\"{col}_parsed\"].notna().sum() > 0:\n",
    "            date_col = f\"{col}_parsed\"\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        print(f\"Analizando columna: {date_col}\")\n",
    "        \n",
    "        # Distribuci√≥n por mes\n",
    "        monthly_dist = df[date_col].dt.month.value_counts().sort_index()\n",
    "        print(f\"\\nüìÖ Distribuci√≥n por mes:\")\n",
    "        for month, count in monthly_dist.items():\n",
    "            month_name = pd.to_datetime(f\"2023-{month:02d}-01\").strftime('%B')\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {month:2d} ({month_name[:3]}): {count:,} llamadas ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Distribuci√≥n por d√≠a de la semana\n",
    "        weekday_dist = df[f\"{date_col.replace('_parsed', '')}_weekday\"].value_counts()\n",
    "        print(f\"\\nüìä Distribuci√≥n por d√≠a de la semana:\")\n",
    "        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        for day in weekday_order:\n",
    "            if day in weekday_dist.index:\n",
    "                count = weekday_dist[day]\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"  {day}: {count:,} llamadas ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se pudieron procesar campos de fecha para an√°lisis temporal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f9f69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. üìà Visualizaciones Exploratorias\n",
    "\n",
    "### Gr√°ficos Iniciales para Comprensi√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c766def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de distribuciones b√°sicas\n",
    "print(\"üìä CREANDO VISUALIZACIONES EXPLORATORIAS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Distribuci√≥n de variables num√©ricas\n",
    "if numeric_cols:\n",
    "    print(\"\\nüìà Distribuciones de Variables Num√©ricas\")\n",
    "    \n",
    "    # Calcular n√∫mero de subplots necesarios\n",
    "    n_numeric = len(numeric_cols)\n",
    "    if n_numeric > 0:\n",
    "        # Crear subplots\n",
    "        n_cols = min(3, n_numeric)\n",
    "        n_rows = (n_numeric + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_numeric == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols[:9]):  # L√≠mite de 9 variables\n",
    "            ax = axes[i] if i < len(axes) else None\n",
    "            if ax is not None:\n",
    "                # Histograma con KDE\n",
    "                df[col].hist(bins=50, alpha=0.7, ax=ax)\n",
    "                ax.set_title(f'Distribuci√≥n de {col}')\n",
    "                ax.set_xlabel(col)\n",
    "                ax.set_ylabel('Frecuencia')\n",
    "                \n",
    "                # Agregar estad√≠sticas b√°sicas como texto\n",
    "                mean_val = df[col].mean()\n",
    "                median_val = df[col].median()\n",
    "                ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Media: {mean_val:.2f}')\n",
    "                ax.axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Mediana: {median_val:.2f}')\n",
    "                ax.legend(fontsize=8)\n",
    "        \n",
    "        # Ocultar subplots vac√≠os\n",
    "        for i in range(n_numeric, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 2. Distribuci√≥n de variables categ√≥ricas principales\n",
    "if categorical_cols:\n",
    "    print(\"\\nüìä Distribuciones de Variables Categ√≥ricas\")\n",
    "    \n",
    "    # Seleccionar las variables categ√≥ricas m√°s importantes (con menos valores √∫nicos)\n",
    "    cat_analysis = []\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        if 2 <= unique_count <= 20:  # Solo variables con 2-20 valores √∫nicos\n",
    "            cat_analysis.append((col, unique_count))\n",
    "    \n",
    "    # Ordenar por n√∫mero de valores √∫nicos\n",
    "    cat_analysis.sort(key=lambda x: x[1])\n",
    "    \n",
    "    if cat_analysis:\n",
    "        # Mostrar top 6 variables categ√≥ricas\n",
    "        for i, (col, unique_count) in enumerate(cat_analysis[:6]):\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Contar valores y calcular porcentajes\n",
    "            value_counts = df[col].value_counts()\n",
    "            \n",
    "            # Crear gr√°fico de barras\n",
    "            ax = value_counts.plot(kind='bar', color=plt.cm.Set3(np.linspace(0, 1, len(value_counts))))\n",
    "            plt.title(f'Distribuci√≥n de {col} ({unique_count} valores √∫nicos)', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel(col, fontsize=12)\n",
    "            plt.ylabel('Frecuencia', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            # Agregar porcentajes en las barras\n",
    "            total = len(df)\n",
    "            for j, (value, count) in enumerate(value_counts.items()):\n",
    "                percentage = (count / total) * 100\n",
    "                ax.text(j, count + max(value_counts) * 0.01, f'{percentage:.1f}%', \n",
    "                       ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            if i >= 3:  # L√≠mite de 4 gr√°ficos\n",
    "                remaining = len(cat_analysis) - 4\n",
    "                if remaining > 0:\n",
    "                    print(f\"\\nüí° Hay {remaining} variables categ√≥ricas adicionales para analizar posteriormente\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se encontraron variables categ√≥ricas adecuadas para visualizaci√≥n (2-20 valores √∫nicos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones temporales avanzadas\n",
    "if date_cols:\n",
    "    print(\"\\nüìÖ VISUALIZACIONES TEMPORALES\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Buscar la columna de fecha procesada\n",
    "    date_col = None\n",
    "    for col, _ in date_cols:\n",
    "        if f\"{col}_parsed\" in df.columns and df[f\"{col}_parsed\"].notna().sum() > 100:\n",
    "            date_col = f\"{col}_parsed\"\n",
    "            original_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        print(f\"Creando visualizaciones temporales para: {date_col}\")\n",
    "        \n",
    "        # 1. Serie temporal de llamadas por d√≠a\n",
    "        daily_calls = df.groupby(df[date_col].dt.date).size()\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        daily_calls.plot(kind='line', color='steelblue', linewidth=2)\n",
    "        plt.title('Volumen de Llamadas por D√≠a', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Fecha', fontsize=12)\n",
    "        plt.ylabel('N√∫mero de Llamadas', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Agregar l√≠nea de tendencia\n",
    "        z = np.polyfit(range(len(daily_calls)), daily_calls.values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(daily_calls.index, p(range(len(daily_calls))), \"r--\", alpha=0.8, label='Tendencia')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Distribuci√≥n por mes usando plotly\n",
    "        monthly_data = df[f\"{original_col}_month\"].value_counts().sort_index()\n",
    "        month_names = [pd.to_datetime(f\"2023-{m:02d}-01\").strftime('%B') for m in monthly_data.index]\n",
    "        \n",
    "        fig = px.bar(\n",
    "            x=month_names,\n",
    "            y=monthly_data.values,\n",
    "            title='Distribuci√≥n de Llamadas por Mes',\n",
    "            labels={'x': 'Mes', 'y': 'N√∫mero de Llamadas'},\n",
    "            color=monthly_data.values,\n",
    "            color_continuous_scale='viridis'\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            showlegend=False,\n",
    "            xaxis_title=\"Mes\",\n",
    "            yaxis_title=\"N√∫mero de Llamadas\"\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # 3. Heatmap por d√≠a de la semana y mes\n",
    "        if f\"{original_col}_weekday\" in df.columns:\n",
    "            # Crear tabla pivote\n",
    "            heatmap_data = df.groupby([f\"{original_col}_month\", f\"{original_col}_weekday\"]).size().unstack(fill_value=0)\n",
    "            \n",
    "            # Reordenar d√≠as de la semana\n",
    "            weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            heatmap_data = heatmap_data.reindex(columns=[day for day in weekday_order if day in heatmap_data.columns])\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(\n",
    "                heatmap_data,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'N√∫mero de Llamadas'}\n",
    "            )\n",
    "            plt.title('Heatmap: Llamadas por Mes y D√≠a de la Semana', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('D√≠a de la Semana', fontsize=12)\n",
    "            plt.ylabel('Mes', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se pudo crear visualizaciones temporales (no hay fechas v√°lidas procesadas)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay campos de fecha para crear visualizaciones temporales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ebdf8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. üìã Resumen y Conclusiones Iniciales\n",
    "\n",
    "### Hallazgos Principales del An√°lisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen ejecutivo del an√°lisis\n",
    "print(\"üìä RESUMEN EJECUTIVO DEL AN√ÅLISIS EXPLORATORIO\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Informaci√≥n b√°sica del dataset\n",
    "print(f\"\\nüìà DIMENSIONES DEL DATASET:\")\n",
    "print(f\"   ‚Ä¢ Total de registros: {df.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Total de columnas: {df.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Per√≠odo analizado: A√±o 1999 (Call Center Anonymous Bank)\")\n",
    "\n",
    "# Calidad de datos\n",
    "total_missing = df.isnull().sum().sum()\n",
    "total_cells = df.shape[0] * df.shape[1]\n",
    "missing_percentage = (total_missing / total_cells) * 100\n",
    "\n",
    "print(f\"\\nüßπ CALIDAD DE DATOS:\")\n",
    "print(f\"   ‚Ä¢ Completitud: {100 - missing_percentage:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Valores faltantes: {total_missing:,} de {total_cells:,} celdas\")\n",
    "if missing_percentage < 1:\n",
    "    print(f\"   ‚Ä¢ Estado: ‚úÖ Excelente calidad de datos\")\n",
    "elif missing_percentage < 5:\n",
    "    print(f\"   ‚Ä¢ Estado: ‚úÖ Buena calidad de datos\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Estado: ‚ö†Ô∏è Requiere limpieza de datos\")\n",
    "\n",
    "# Tipos de variables\n",
    "print(f\"\\nüìä TIPOS DE VARIABLES:\")\n",
    "print(f\"   ‚Ä¢ Variables num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"   ‚Ä¢ Variables categ√≥ricas: {len(categorical_cols)}\")\n",
    "if date_cols:\n",
    "    print(f\"   ‚Ä¢ Variables temporales: {len(date_cols)}\")\n",
    "\n",
    "# Campos espec√≠ficos del call center identificados\n",
    "if existing_fields:\n",
    "    print(f\"\\nüìû CAMPOS DE CALL CENTER IDENTIFICADOS:\")\n",
    "    for field, description in existing_fields.items():\n",
    "        unique_vals = df[field].nunique()\n",
    "        print(f\"   ‚Ä¢ {field}: {description} ({unique_vals:,} valores √∫nicos)\")\n",
    "\n",
    "# An√°lisis temporal (si disponible)\n",
    "if date_cols:\n",
    "    date_col = None\n",
    "    for col, _ in date_cols:\n",
    "        if f\"{col}_parsed\" in df.columns:\n",
    "            date_col = f\"{col}_parsed\"\n",
    "            break\n",
    "    \n",
    "    if date_col and df[date_col].notna().sum() > 0:\n",
    "        min_date = df[date_col].min()\n",
    "        max_date = df[date_col].max()\n",
    "        duration = max_date - min_date\n",
    "        \n",
    "        print(f\"\\nüìÖ COBERTURA TEMPORAL:\")\n",
    "        print(f\"   ‚Ä¢ Fecha inicio: {min_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ‚Ä¢ Fecha fin: {max_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   ‚Ä¢ Duraci√≥n: {duration.days} d√≠as\")\n",
    "        \n",
    "        # Promedio de llamadas por d√≠a\n",
    "        avg_calls_per_day = len(df) / duration.days if duration.days > 0 else 0\n",
    "        print(f\"   ‚Ä¢ Promedio de llamadas/d√≠a: {avg_calls_per_day:.0f}\")\n",
    "\n",
    "print(f\"\\n\\nüéØ PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
    "print(f\"   1. üßπ Limpieza y preparaci√≥n de datos\")\n",
    "print(f\"   2. üîç An√°lisis detallado de patrones temporales\")\n",
    "print(f\"   3. ‚è±Ô∏è An√°lisis de tiempos de servicio y eficiencia\")\n",
    "print(f\"   4. üë• An√°lisis de comportamiento de clientes\")\n",
    "print(f\"   5. üìà Desarrollo de KPIs operacionales\")\n",
    "print(f\"   6. ü§ñ Modelado predictivo para demanda de llamadas\")\n",
    "print(f\"   7. üìä Creaci√≥n de dashboard interactivo\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 55)\n",
    "print(f\"‚úÖ AN√ÅLISIS EXPLORATORIO INICIAL COMPLETADO\")\n",
    "print(f\"üìù Documentar hallazgos y proceder con an√°lisis detallado\")\n",
    "print(f\"=\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar informaci√≥n para los siguientes notebooks\n",
    "print(\"\\nüîÑ PREPARACI√ìN PARA AN√ÅLISIS POSTERIOR\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Guardar informaci√≥n clave para pr√≥ximos an√°lisis\n",
    "analysis_summary = {\n",
    "    'dataset_info': {\n",
    "        'rows': df.shape[0],\n",
    "        'columns': df.shape[1],\n",
    "        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "        'missing_percentage': missing_percentage\n",
    "    },\n",
    "    'column_types': {\n",
    "        'numeric': numeric_cols,\n",
    "        'categorical': categorical_cols,\n",
    "        'datetime': [col for col, _ in date_cols] if date_cols else []\n",
    "    },\n",
    "    'call_center_fields': existing_fields if 'existing_fields' in locals() else {},\n",
    "    'time_fields': time_cols if 'time_cols' in locals() else []\n",
    "}\n",
    "\n",
    "# Mostrar resumen para pr√≥ximos notebooks\n",
    "print(\"üìã INFORMACI√ìN PARA PR√ìXIMOS AN√ÅLISIS:\")\n",
    "print(f\"   ‚Ä¢ Dataset cargado: {analysis_summary['dataset_info']['rows']:,} registros\")\n",
    "print(f\"   ‚Ä¢ Campos num√©ricos para an√°lisis estad√≠stico: {len(analysis_summary['column_types']['numeric'])}\")\n",
    "print(f\"   ‚Ä¢ Campos categ√≥ricos para segmentaci√≥n: {len(analysis_summary['column_types']['categorical'])}\")\n",
    "print(f\"   ‚Ä¢ Campos temporales para an√°lisis de tendencias: {len(analysis_summary['column_types']['datetime'])}\")\n",
    "\n",
    "# Verificar si hay datos suficientes para diferentes tipos de an√°lisis\n",
    "if len(df) > 10000:\n",
    "    print(\"\\n‚úÖ SUFICIENTES DATOS PARA:\")\n",
    "    print(\"   ‚Ä¢ An√°lisis estad√≠stico robusto\")\n",
    "    print(\"   ‚Ä¢ Modelado predictivo\")\n",
    "    print(\"   ‚Ä¢ Segmentaci√≥n de clientes\")\n",
    "    print(\"   ‚Ä¢ An√°lisis de series temporales\")\n",
    "elif len(df) > 1000:\n",
    "    print(\"\\n‚ö†Ô∏è DATOS MODERADOS - ADECUADOS PARA:\")\n",
    "    print(\"   ‚Ä¢ An√°lisis descriptivo\")\n",
    "    print(\"   ‚Ä¢ Modelado b√°sico\")\n",
    "    print(\"   ‚Ä¢ Visualizaciones\")\n",
    "else:\n",
    "    print(\"\\n‚ùå DATOS LIMITADOS - CONSIDERAR:\")\n",
    "    print(\"   ‚Ä¢ An√°lisis exploratorio √∫nicamente\")\n",
    "    print(\"   ‚Ä¢ Validaci√≥n de metodolog√≠a\")\n",
    "\n",
    "print(\"\\nüìö NOTEBOOKS RECOMENDADOS A CREAR:\")\n",
    "print(\"   1. 02_data_cleaning_transformation.ipynb - Limpieza y transformaci√≥n\")\n",
    "print(\"   2. 03_exploratory_data_analysis.ipynb - EDA detallado\")\n",
    "print(\"   3. 04_temporal_analysis.ipynb - An√°lisis temporal avanzado\")\n",
    "print(\"   4. 05_performance_metrics.ipynb - KPIs y m√©tricas operacionales\")\n",
    "print(\"   5. 06_customer_segmentation.ipynb - Segmentaci√≥n de clientes\")\n",
    "print(\"   6. 07_predictive_modeling.ipynb - Modelos predictivos\")\n",
    "print(\"   7. 08_dashboard_preparation.ipynb - Preparaci√≥n para dashboards\")\n",
    "\n",
    "print(\"\\nüéØ SIGUIENTE PASO INMEDIATO:\")\n",
    "print(\"   Ejecutar notebook de limpieza y transformaci√≥n de datos\")\n",
    "print(\"   para preparar el dataset para an√°lisis avanzados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d071a25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Conclusiones y Siguientes Pasos\n",
    "\n",
    "### ‚úÖ Lo que hemos logrado\n",
    "\n",
    "1. **Carga exitosa** del dataset del Call Center (a√±o 1999)\n",
    "2. **An√°lisis de estructura** y calidad de datos\n",
    "3. **Identificaci√≥n de campos clave** para an√°lisis de call center\n",
    "4. **Procesamiento temporal** b√°sico\n",
    "5. **Visualizaciones exploratorias** iniciales\n",
    "6. **Resumen ejecutivo** de hallazgos principales\n",
    "\n",
    "### üîç Hallazgos Clave\n",
    "\n",
    "- Dataset robusto con informaci√≥n detallada de llamadas\n",
    "- Calidad de datos generalmente buena\n",
    "- Campos espec√≠ficos de call center identificados\n",
    "- Patrones temporales visibles para an√°lisis posterior\n",
    "- Oportunidades claras para optimizaci√≥n operacional\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "1. **Limpieza de Datos**: Procesamiento y preparaci√≥n para an√°lisis\n",
    "2. **An√°lisis Temporal Detallado**: Patrones por hora, d√≠a, semana, mes\n",
    "3. **M√©tricas Operacionales**: KPIs de eficiencia y calidad de servicio\n",
    "4. **Segmentaci√≥n**: An√°lisis de tipos de clientes y servicios\n",
    "5. **Modelado Predictivo**: Forecasting de demanda y optimizaci√≥n\n",
    "6. **Dashboard**: Visualizaciones interactivas para stakeholders\n",
    "\n",
    "### üìä Valor del Proyecto\n",
    "\n",
    "Este an√°lisis proporcionar√° insights accionables para:\n",
    "- **Optimizar staffing** y recursos del call center\n",
    "- **Mejorar tiempos de respuesta** y calidad de servicio\n",
    "- **Predecir demanda** y planificar capacidad\n",
    "- **Identificar oportunidades** de mejora operacional\n",
    "\n",
    "---\n",
    "\n",
    "**¬°El an√°lisis exploratorio inicial est√° completo! Procede con la limpieza y transformaci√≥n de datos. üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "call_center_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
