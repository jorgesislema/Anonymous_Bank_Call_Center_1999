{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a099ef",
   "metadata": {},
   "source": [
    "# ü§ñ Machine Learning y Modelos Predictivos\n",
    "\n",
    "## Objetivo\n",
    "Este notebook desarrolla **modelos de machine learning avanzados** para predecir y optimizar las operaciones del call center, incluyendo predicci√≥n de abandonos, clasificaci√≥n de llamadas y optimizaci√≥n de recursos.\n",
    "\n",
    "## Contenido del An√°lisis\n",
    "1. **Preparaci√≥n de Datos para ML**\n",
    "2. **Predicci√≥n de Abandonos (Clasificaci√≥n)**\n",
    "3. **Predicci√≥n de Tiempo de Servicio (Regresi√≥n)**\n",
    "4. **Clasificaci√≥n de Tipos de Llamada**\n",
    "5. **Optimizaci√≥n de Asignaci√≥n de Recursos**\n",
    "6. **Detecci√≥n de Anomal√≠as**\n",
    "7. **Modelos de Pron√≥stico de Demanda**\n",
    "8. **Evaluaci√≥n y Comparaci√≥n de Modelos**\n",
    "9. **Implementaci√≥n y Deployment**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa740cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as para machine learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import sys\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Configuraciones\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Configuraci√≥n de plotly\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "sys.path.append('../02_src')\n",
    "from feature_engineering import FeatureEngineer\n",
    "\n",
    "print(\"ü§ñ Librer√≠as de Machine Learning importadas exitosamente\")\n",
    "print(f\" Pandas versi√≥n: {pd.__version__}\")\n",
    "print(f\" Scikit-learn disponible\")\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0fc74",
   "metadata": {},
   "source": [
    "## 1. Preparaci√≥n de Datos para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef237c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos limpios\n",
    "print(\" Cargando datos para machine learning...\")\n",
    "df = pd.read_parquet('../00_data/processed/call_center_clean.parquet')\n",
    "\n",
    "print(f\" Datos cargados: {df.shape[0]:,} filas x {df.shape[1]} columnas\")\n",
    "\n",
    "# Preparar caracter√≠sticas temporales avanzadas\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['vru_entry_dt'] = pd.to_datetime(df['vru_entry'])\n",
    "df['hour'] = df['vru_entry_dt'].dt.hour\n",
    "df['minute'] = df['vru_entry_dt'].dt.minute\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['is_weekend'] = (df['date'].dt.weekday >= 5).astype(int)\n",
    "df['is_monday'] = (df['date'].dt.weekday == 0).astype(int)\n",
    "df['is_friday'] = (df['date'].dt.weekday == 4).astype(int)\n",
    "\n",
    "# Caracter√≠sticas de carga de trabajo\n",
    "hourly_load = df.groupby(['date', 'hour'])['call_id'].count().reset_index()\n",
    "hourly_load.columns = ['date', 'hour', 'hourly_calls']\n",
    "df = df.merge(hourly_load, on=['date', 'hour'], how='left')\n",
    "\n",
    "# Caracter√≠sticas de cliente (para clientes conocidos)\n",
    "customer_history = df[df['customer_id'].notna()].groupby('customer_id').agg({\n",
    "    'call_id': 'count',\n",
    "    'outcome': lambda x: (x == 'HANG').sum(),\n",
    "    'ser_time': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "customer_history.columns = ['customer_call_count', 'customer_hang_count', 'customer_avg_service_time']\n",
    "customer_history['customer_hang_rate'] = (customer_history['customer_hang_count'] / customer_history['customer_call_count'] * 100).round(2)\n",
    "\n",
    "df = df.merge(customer_history, left_on='customer_id', right_index=True, how='left')\n",
    "\n",
    "# Rellenar NaN para clientes nuevos/desconocidos\n",
    "df['customer_call_count'] = df['customer_call_count'].fillna(1)\n",
    "df['customer_hang_count'] = df['customer_hang_count'].fillna(0)\n",
    "df['customer_avg_service_time'] = df['customer_avg_service_time'].fillna(df['ser_time'].median())\n",
    "df['customer_hang_rate'] = df['customer_hang_rate'].fillna(0)\n",
    "\n",
    "# Crear caracter√≠sticas de eficiencia\n",
    "df['efficiency_ratio'] = df['ser_time'] / (df['ser_time'] + df['q_time'] + df['vru_time'] + 1)\n",
    "df['total_time'] = df['ser_time'] + df['q_time'] + df['vru_time']\n",
    "df['wait_time_ratio'] = (df['q_time'] + df['vru_time']) / (df['total_time'] + 1)\n",
    "\n",
    "# Codificar variables categ√≥ricas\n",
    "le_type = LabelEncoder()\n",
    "le_vru_line = LabelEncoder()\n",
    "le_server = LabelEncoder()\n",
    "\n",
    "df['type_encoded'] = le_type.fit_transform(df['type'])\n",
    "df['vru_line_encoded'] = le_vru_line.fit_transform(df['vru.line'])\n",
    "df['server_encoded'] = le_server.fit_transform(df['server'])\n",
    "\n",
    "print(\"Ô∏è Caracter√≠sticas de ML preparadas\")\n",
    "print(f\" Total de caracter√≠sticas: {df.shape[1]}\")\n",
    "\n",
    "# Mostrar estad√≠sticas de outcomes\n",
    "outcome_counts = df['outcome'].value_counts()\n",
    "print(\"\\n Distribuci√≥n de outcomes:\")\n",
    "for outcome, count in outcome_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {outcome}: {count:,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e023f5",
   "metadata": {},
   "source": [
    "## 2. Modelo de Predicci√≥n de Abandonos (Clasificaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para predicci√≥n de abandonos\n",
    "print(\" Desarrollando modelo de predicci√≥n de abandonos...\")\n",
    "\n",
    "# Variables objetivo: 1 si es HANG, 0 si no\n",
    "df['is_hang'] = (df['outcome'] == 'HANG').astype(int)\n",
    "\n",
    "# Seleccionar caracter√≠sticas para el modelo\n",
    "feature_columns = [\n",
    "    'priority', 'type_encoded', 'vru_line_encoded', 'vru_time', 'q_time',\n",
    "    'hour', 'minute', 'day_of_week', 'month', 'is_weekend', 'is_monday', 'is_friday',\n",
    "    'hourly_calls', 'customer_call_count', 'customer_hang_rate', 'customer_avg_service_time',\n",
    "    'efficiency_ratio', 'wait_time_ratio', 'server_encoded'\n",
    "]\n",
    "\n",
    "# Preparar dataset\n",
    "X = df[feature_columns].copy()\n",
    "y = df['is_hang']\n",
    "\n",
    "# Manejar valores faltantes\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Escalar caracter√≠sticas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\" Dataset preparado: {X_train.shape[0]:,} entrenamiento, {X_test.shape[0]:,} prueba\")\n",
    "print(f\" Balance de clases - Abandono: {y_train.mean():.3f}, No abandono: {1-y_train.mean():.3f}\")\n",
    "\n",
    "# Limitar el tama√±o del dataset para pruebas r√°pidas\n",
    "MAX_ROWS = 10000\n",
    "if len(df) > MAX_ROWS:\n",
    "    print(f\"‚ö†Ô∏è Usando solo una muestra de {MAX_ROWS} filas para acelerar el entrenamiento de modelos.\")\n",
    "    df = df.sample(n=MAX_ROWS, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Entrenar m√∫ltiples modelos (simplificados para ejecuci√≥n r√°pida)\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, max_depth=7),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=500),\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "print(\"\\n Entrenando modelos...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"  Entrenando {name}...\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    if name in ['SVM', 'Neural Network', 'Logistic Regression']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "print(\"\\n Resultados de modelos de predicci√≥n de abandonos:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Modelo': list(model_results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in model_results.values()],\n",
    "    'Precision': [r['precision'] for r in model_results.values()],\n",
    "    'Recall': [r['recall'] for r in model_results.values()],\n",
    "    'F1-Score': [r['f1'] for r in model_results.values()],\n",
    "    'AUC': [r['auc'] for r in model_results.values()]\n",
    "}).round(4)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo\n",
    "best_model_name = results_df.loc[results_df['AUC'].idxmax(), 'Modelo']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n Mejor modelo: {best_model_name} (AUC: {results_df['AUC'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de resultados del modelo de abandonos\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Comparaci√≥n de Modelos (AUC)', 'Curva ROC - Mejor Modelo',\n",
    "        'Matriz de Confusi√≥n', 'Importancia de Caracter√≠sticas'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"heatmap\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Comparaci√≥n de modelos\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=results_df['Modelo'],\n",
    "        y=results_df['AUC'],\n",
    "        name='AUC Score',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Curva ROC del mejor modelo\n",
    "best_proba = model_results[best_model_name]['probabilities']\n",
    "fpr, tpr, _ = roc_curve(y_test, best_proba)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'ROC {best_model_name}',\n",
    "        line=dict(color='blue', width=3)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# L√≠nea diagonal de referencia\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Random',\n",
    "        line=dict(color='red', dash='dash')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Matriz de confusi√≥n\n",
    "best_pred = model_results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm,\n",
    "        x=['No Abandono', 'Abandono'],\n",
    "        y=['No Abandono', 'Abandono'],\n",
    "        colorscale='Blues',\n",
    "        showscale=True,\n",
    "        text=cm,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16}\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Importancia de caracter√≠sticas (solo para Random Forest)\n",
    "if best_model_name == 'Random Forest':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True).tail(10)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=feature_importance['importance'],\n",
    "            y=feature_importance['feature'],\n",
    "            orientation='h',\n",
    "            name='Importancia',\n",
    "            marker_color='green'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\" Modelo de Predicci√≥n de Abandonos - Resultados\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Modelo\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"AUC Score\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Tasa de Falsos Positivos\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Tasa de Verdaderos Positivos\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Predicci√≥n\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Real\", row=2, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\" Visualizaci√≥n del modelo de abandonos creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f13c2",
   "metadata": {},
   "source": [
    "## 3. Modelo de Predicci√≥n de Tiempo de Servicio (Regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para predicci√≥n de tiempo de servicio\n",
    "print(\"‚è±Ô∏è Desarrollando modelo de predicci√≥n de tiempo de servicio...\")\n",
    "\n",
    "# Filtrar solo llamadas atendidas (AGENT)\n",
    "df_agent = df[df['outcome'] == 'AGENT'].copy()\n",
    "\n",
    "# Variables para predicci√≥n (excluyendo ser_time que es el target)\n",
    "regression_features = [\n",
    "    'priority', 'type_encoded', 'vru_line_encoded', 'vru_time', 'q_time',\n",
    "    'hour', 'minute', 'day_of_week', 'month', 'is_weekend',\n",
    "    'hourly_calls', 'customer_call_count', 'customer_hang_rate',\n",
    "    'server_encoded'\n",
    "]\n",
    "\n",
    "# Preparar dataset\n",
    "X_reg = df_agent[regression_features].copy()\n",
    "y_reg = df_agent['ser_time']\n",
    "\n",
    "# Manejar valores faltantes\n",
    "X_reg = X_reg.fillna(X_reg.median())\n",
    "\n",
    "# Remover outliers extremos (m√°s de 3 desviaciones est√°ndar)\n",
    "z_scores = np.abs(stats.zscore(y_reg))\n",
    "outlier_mask = z_scores < 3\n",
    "X_reg = X_reg[outlier_mask]\n",
    "y_reg = y_reg[outlier_mask]\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Escalar caracter√≠sticas\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "print(f\" Dataset para regresi√≥n: {X_train_reg.shape[0]:,} entrenamiento, {X_test_reg.shape[0]:,} prueba\")\n",
    "print(f\" Tiempo promedio de servicio: {y_train_reg.mean():.2f} segundos\")\n",
    "\n",
    "# Entrenar modelos de regresi√≥n (solo Random Forest y Linear Regression)\n",
    "regression_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42, max_depth=7),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "print(\"\\n Entrenando modelos de regresi√≥n...\")\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"  Entrenando {name}...\")\n",
    "    # Ambos modelos usan datos escalados\n",
    "    model.fit(X_train_reg_scaled, y_train_reg)\n",
    "    y_pred_reg = model.predict(X_test_reg_scaled)\n",
    "    # Calcular m√©tricas\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mse)\n",
    "    regression_results[name] = {\n",
    "        'model': model,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'predictions': y_pred_reg\n",
    "    }\n",
    "\n",
    "print(\"\\n Resultados de modelos de predicci√≥n de tiempo:\")\n",
    "reg_results_df = pd.DataFrame({\n",
    "    'Modelo': list(regression_results.keys()),\n",
    "    'R¬≤': [r['r2'] for r in regression_results.values()],\n",
    "    'RMSE': [r['rmse'] for r in regression_results.values()],\n",
    "    'MAE': [r['mae'] for r in regression_results.values()]\n",
    "}).round(4)\n",
    "\n",
    "print(reg_results_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor modelo de regresi√≥n\n",
    "best_reg_model_name = reg_results_df.loc[reg_results_df['R¬≤'].idxmax(), 'Modelo']\n",
    "best_reg_model = regression_results[best_reg_model_name]['model']\n",
    "\n",
    "print(f\"\\n Mejor modelo de regresi√≥n: {best_reg_model_name} (R¬≤: {reg_results_df['R¬≤'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26e4dd",
   "metadata": {},
   "source": [
    "## 4. Detecci√≥n de Anomal√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecci√≥n de anomal√≠as en operaciones del call center\n",
    "print(\" Desarrollando sistema de detecci√≥n de anomal√≠as...\")\n",
    "\n",
    "# Preparar datos para detecci√≥n de anomal√≠as\n",
    "anomaly_features = [\n",
    "    'ser_time', 'vru_time', 'q_time', 'total_time', 'efficiency_ratio',\n",
    "    'wait_time_ratio', 'hourly_calls', 'priority'\n",
    "]\n",
    "\n",
    "X_anomaly = df[anomaly_features].copy()\n",
    "X_anomaly = X_anomaly.fillna(X_anomaly.median())\n",
    "\n",
    "# Escalar datos\n",
    "scaler_anomaly = StandardScaler()\n",
    "X_anomaly_scaled = scaler_anomaly.fit_transform(X_anomaly)\n",
    "\n",
    "# Aplicar Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(X_anomaly_scaled)\n",
    "\n",
    "# -1 indica anomal√≠a, 1 indica normal\n",
    "df['is_anomaly'] = (anomaly_labels == -1).astype(int)\n",
    "anomaly_count = df['is_anomaly'].sum()\n",
    "anomaly_percentage = (anomaly_count / len(df)) * 100\n",
    "\n",
    "print(f\" Anomal√≠as detectadas: {anomaly_count:,} ({anomaly_percentage:.2f}%)\")\n",
    "\n",
    "# Analizar caracter√≠sticas de las anomal√≠as\n",
    "normal_calls = df[df['is_anomaly'] == 0]\n",
    "anomalous_calls = df[df['is_anomaly'] == 1]\n",
    "\n",
    "print(\"\\n Comparaci√≥n Normal vs Anomal√≠as:\")\n",
    "comparison_metrics = ['ser_time', 'vru_time', 'q_time', 'total_time']\n",
    "for metric in comparison_metrics:\n",
    "    normal_avg = normal_calls[metric].mean()\n",
    "    anomaly_avg = anomalous_calls[metric].mean()\n",
    "    print(f\"  {metric}: Normal={normal_avg:.2f}s, Anomal√≠a={anomaly_avg:.2f}s\")\n",
    "\n",
    "# Distribuci√≥n de anomal√≠as por outcome\n",
    "anomaly_by_outcome = df.groupby(['outcome', 'is_anomaly']).size().unstack(fill_value=0)\n",
    "anomaly_rates = (anomaly_by_outcome[1] / (anomaly_by_outcome[0] + anomaly_by_outcome[1]) * 100).round(2)\n",
    "\n",
    "print(\"\\n Tasa de anomal√≠as por tipo de resultado:\")\n",
    "for outcome, rate in anomaly_rates.items():\n",
    "    print(f\"  {outcome}: {rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72393ef",
   "metadata": {},
   "source": [
    "## 5. Optimizaci√≥n de Recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema de optimizaci√≥n de recursos\n",
    "print(\" Desarrollando sistema de optimizaci√≥n de recursos...\")\n",
    "\n",
    "# An√°lisis de carga por hora y d√≠a\n",
    "resource_analysis = df.groupby(['day_of_week', 'hour']).agg({\n",
    "    'call_id': 'count',\n",
    "    'ser_time': 'mean',\n",
    "    'q_time': 'mean',\n",
    "    'outcome': lambda x: (x == 'HANG').sum(),\n",
    "    'server': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "resource_analysis.columns = ['total_calls', 'avg_service_time', 'avg_queue_time', 'hangs', 'servers_used']\n",
    "resource_analysis['hang_rate'] = (resource_analysis['hangs'] / resource_analysis['total_calls'] * 100).round(2)\n",
    "resource_analysis['calls_per_server'] = (resource_analysis['total_calls'] / resource_analysis['servers_used']).round(2)\n",
    "\n",
    "# Calcular m√©tricas de eficiencia\n",
    "resource_analysis['efficiency_score'] = (\n",
    "    (100 - resource_analysis['hang_rate']) * \n",
    "    (1 / (1 + resource_analysis['avg_queue_time'] / 60))  # Penalizar tiempos de espera largos\n",
    ").round(2)\n",
    "\n",
    "# Identificar per√≠odos cr√≠ticos\n",
    "high_load_threshold = resource_analysis['total_calls'].quantile(0.9)\n",
    "high_hang_threshold = resource_analysis['hang_rate'].quantile(0.9)\n",
    "\n",
    "resource_analysis['is_high_load'] = (resource_analysis['total_calls'] >= high_load_threshold)\n",
    "resource_analysis['is_high_hang'] = (resource_analysis['hang_rate'] >= high_hang_threshold)\n",
    "resource_analysis['needs_attention'] = (resource_analysis['is_high_load'] | resource_analysis['is_high_hang'])\n",
    "\n",
    "critical_periods = resource_analysis[resource_analysis['needs_attention']]\n",
    "\n",
    "print(f\" An√°lisis de recursos completado\")\n",
    "print(f\"‚ö†Ô∏è Per√≠odos cr√≠ticos identificados: {len(critical_periods)}\")\n",
    "\n",
    "if len(critical_periods) > 0:\n",
    "    print(\"\\n Top 5 per√≠odos que requieren atenci√≥n:\")\n",
    "    top_critical = critical_periods.nlargest(5, 'hang_rate')[['total_calls', 'hang_rate', 'servers_used', 'efficiency_score']]\n",
    "    print(top_critical.to_string())\n",
    "\n",
    "# Recomendaciones de staffing\n",
    "def calculate_optimal_servers(calls, target_hang_rate=10, avg_calls_per_server=50):\n",
    "    \"\"\"Calcular n√∫mero √≥ptimo de servidores basado en carga y objetivo de hang rate\"\"\"\n",
    "    base_servers = max(1, calls // avg_calls_per_server)\n",
    "    # Ajustar basado en hang rate objetivo\n",
    "    adjustment_factor = 1.2 if calls > avg_calls_per_server else 1.0\n",
    "    return max(1, int(base_servers * adjustment_factor))\n",
    "\n",
    "resource_analysis['recommended_servers'] = resource_analysis['total_calls'].apply(\n",
    "    lambda x: calculate_optimal_servers(x)\n",
    ")\n",
    "\n",
    "resource_analysis['server_adjustment'] = resource_analysis['recommended_servers'] - resource_analysis['servers_used']\n",
    "\n",
    "print(\"\\n Recomendaciones de ajuste de personal:\")\n",
    "significant_adjustments = resource_analysis[abs(resource_analysis['server_adjustment']) >= 2]\n",
    "if len(significant_adjustments) > 0:\n",
    "    print(\"Ajustes significativos recomendados:\")\n",
    "    print(significant_adjustments[['servers_used', 'recommended_servers', 'server_adjustment', 'hang_rate']].head(10).to_string())\n",
    "else:\n",
    "    print(\"No se requieren ajustes significativos de personal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4cbf86",
   "metadata": {},
   "source": [
    "## 6. Dashboard de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dashboard interactivo de resultados de ML\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Performance de Modelos de Clasificaci√≥n', 'Predicciones vs Reales (Tiempo)',\n",
    "        'Detecci√≥n de Anomal√≠as por Hora', 'Optimizaci√≥n de Recursos',\n",
    "        'Distribuci√≥n de Anomal√≠as', 'Eficiencia por D√≠a de Semana'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"heatmap\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Performance de modelos de clasificaci√≥n\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=results_df['Modelo'],\n",
    "        y=results_df['F1-Score'],\n",
    "        name='F1-Score',\n",
    "        marker_color='lightcoral'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Predicciones vs Reales para tiempo de servicio\n",
    "best_reg_pred = regression_results[best_reg_model_name]['predictions']\n",
    "sample_indices = np.random.choice(len(y_test_reg), 1000, replace=False)  # Muestra para visualizaci√≥n\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test_reg.iloc[sample_indices],\n",
    "        y=best_reg_pred[sample_indices],\n",
    "        mode='markers',\n",
    "        name='Predicciones',\n",
    "        marker=dict(color='blue', size=4, opacity=0.6)\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Heatmap de anomal√≠as por hora y d√≠a\n",
    "anomaly_heatmap = df.groupby(['day_of_week', 'hour'])['is_anomaly'].mean().unstack(fill_value=0)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=anomaly_heatmap.values,\n",
    "        x=anomaly_heatmap.columns,\n",
    "        y=['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom'],\n",
    "        colorscale='Reds',\n",
    "        showscale=True\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Optimizaci√≥n de recursos - ajustes recomendados\n",
    "adjustment_summary = resource_analysis.groupby('server_adjustment').size().reset_index()\n",
    "adjustment_summary.columns = ['adjustment', 'frequency']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=adjustment_summary['adjustment'],\n",
    "        y=adjustment_summary['frequency'],\n",
    "        name='Frecuencia',\n",
    "        marker_color='lightgreen'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# 5. Distribuci√≥n de anomal√≠as por outcome\n",
    "anomaly_dist = df.groupby('outcome')['is_anomaly'].mean() * 100\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=anomaly_dist.index,\n",
    "        y=anomaly_dist.values,\n",
    "        name='% Anomal√≠as',\n",
    "        marker_color='orange'\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# 6. Eficiencia promedio por d√≠a de semana\n",
    "day_names = ['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom']\n",
    "daily_efficiency = resource_analysis.groupby('day_of_week')['efficiency_score'].mean()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=day_names,\n",
    "        y=daily_efficiency.values,\n",
    "        name='Eficiencia',\n",
    "        marker_color='purple'\n",
    "    ),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1200,\n",
    "    title_text=\"ü§ñ Dashboard de Machine Learning - Call Center Analytics\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Actualizar t√≠tulos de ejes\n",
    "fig.update_xaxes(title_text=\"Modelo\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"F1-Score\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Tiempo Real (s)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Tiempo Predicho (s)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Hora\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"D√≠a de Semana\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Ajuste de Servidores\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frecuencia\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"Tipo de Resultado\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"% Anomal√≠as\", row=3, col=1)\n",
    "fig.update_xaxes(title_text=\"D√≠a de Semana\", row=3, col=2)\n",
    "fig.update_yaxes(title_text=\"Score Eficiencia\", row=3, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\" Dashboard de Machine Learning creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b653d2",
   "metadata": {},
   "source": [
    "## 7. Exportar Modelos y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a64bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos y resultados\n",
    "print(\" Guardando modelos y resultados de machine learning...\")\n",
    "\n",
    "# Crear directorio de salida\n",
    "ml_output_dir = '../03_outputs/machine_learning'\n",
    "models_dir = f'{ml_output_dir}/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Guardar mejor modelo de clasificaci√≥n\n",
    "with open(f'{models_dir}/best_classification_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Guardar mejor modelo de regresi√≥n\n",
    "with open(f'{models_dir}/best_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_reg_model, f)\n",
    "\n",
    "# Guardar escaladores\n",
    "with open(f'{models_dir}/scaler_classification.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(f'{models_dir}/scaler_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_reg, f)\n",
    "\n",
    "# Guardar modelo de detecci√≥n de anomal√≠as\n",
    "with open(f'{models_dir}/anomaly_detection_model.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_forest, f)\n",
    "\n",
    "with open(f'{models_dir}/scaler_anomaly.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_anomaly, f)\n",
    "\n",
    "# Guardar encoders\n",
    "encoders = {\n",
    "    'type_encoder': le_type,\n",
    "    'vru_line_encoder': le_vru_line,\n",
    "    'server_encoder': le_server\n",
    "}\n",
    "\n",
    "with open(f'{models_dir}/encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "# Guardar caracter√≠sticas utilizadas\n",
    "model_features = {\n",
    "    'classification_features': feature_columns,\n",
    "    'regression_features': regression_features,\n",
    "    'anomaly_features': anomaly_features\n",
    "}\n",
    "\n",
    "with open(f'{ml_output_dir}/model_features.json', 'w') as f:\n",
    "    json.dump(model_features, f, indent=2)\n",
    "\n",
    "# Guardar resultados de evaluaci√≥n\n",
    "evaluation_results = {\n",
    "    'classification_results': results_df.to_dict('records'),\n",
    "    'regression_results': reg_results_df.to_dict('records'),\n",
    "    'best_classification_model': best_model_name,\n",
    "    'best_regression_model': best_reg_model_name,\n",
    "    'anomaly_detection': {\n",
    "        'anomalies_detected': int(anomaly_count),\n",
    "        'anomaly_percentage': float(anomaly_percentage)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{ml_output_dir}/evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "# Guardar an√°lisis de recursos\n",
    "resource_analysis.to_csv(f'{ml_output_dir}/resource_optimization.csv')\n",
    "resource_analysis.to_parquet(f'{ml_output_dir}/resource_optimization.parquet')\n",
    "\n",
    "# Crear metadata de modelos\n",
    "model_metadata = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'training_data_shape': {\n",
    "        'classification': {'train': X_train.shape, 'test': X_test.shape},\n",
    "        'regression': {'train': X_train_reg.shape, 'test': X_test_reg.shape}\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'best_classification': {\n",
    "            'name': best_model_name,\n",
    "            'auc': float(results_df.loc[results_df['Modelo'] == best_model_name, 'AUC'].iloc[0])\n",
    "        },\n",
    "        'best_regression': {\n",
    "            'name': best_reg_model_name,\n",
    "            'r2': float(reg_results_df.loc[reg_results_df['Modelo'] == best_reg_model_name, 'R¬≤'].iloc[0])\n",
    "        }\n",
    "    },\n",
    "    'feature_counts': {\n",
    "        'classification': len(feature_columns),\n",
    "        'regression': len(regression_features),\n",
    "        'anomaly': len(anomaly_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{ml_output_dir}/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\" Modelos y resultados guardados en {ml_output_dir}\")\n",
    "print(\" Archivos generados:\")\n",
    "print(\"  üìÇ models/\")\n",
    "print(\"    ‚Ä¢ best_classification_model.pkl\")\n",
    "print(\"    ‚Ä¢ best_regression_model.pkl\")\n",
    "print(\"    ‚Ä¢ anomaly_detection_model.pkl\")\n",
    "print(\"    ‚Ä¢ scaler_*.pkl\")\n",
    "print(\"    ‚Ä¢ encoders.pkl\")\n",
    "print(\"   evaluation_results.json\")\n",
    "print(\"   model_features.json\")\n",
    "print(\"   model_metadata.json\")\n",
    "print(\"   resource_optimization.csv/parquet\")\n",
    "\n",
    "print(\"\\n AN√ÅLISIS DE MACHINE LEARNING COMPLETADO EXITOSAMENTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afe4bb",
   "metadata": {},
   "source": [
    "## 8. Resumen de Resultados de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e949ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final de ML\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ü§ñ REPORTE FINAL - MACHINE LEARNING CALL CENTER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n MODELOS DESARROLLADOS:\")\n",
    "print(f\"   Predicci√≥n de Abandonos: {len(models)} modelos evaluados\")\n",
    "print(f\"    ‚îú‚îÄ Mejor modelo: {best_model_name}\")\n",
    "print(f\"    ‚îú‚îÄ AUC Score: {results_df.loc[results_df['Modelo'] == best_model_name, 'AUC'].iloc[0]:.4f}\")\n",
    "print(f\"    ‚îî‚îÄ F1-Score: {results_df.loc[results_df['Modelo'] == best_model_name, 'F1-Score'].iloc[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n  ‚è±Ô∏è Predicci√≥n de Tiempo de Servicio: {len(regression_models)} modelos evaluados\")\n",
    "print(f\"    ‚îú‚îÄ Mejor modelo: {best_reg_model_name}\")\n",
    "print(f\"    ‚îú‚îÄ R¬≤ Score: {reg_results_df.loc[reg_results_df['Modelo'] == best_reg_model_name, 'R¬≤'].iloc[0]:.4f}\")\n",
    "print(f\"    ‚îî‚îÄ RMSE: {reg_results_df.loc[reg_results_df['Modelo'] == best_reg_model_name, 'RMSE'].iloc[0]:.2f}s\")\n",
    "\n",
    "print(f\"\\n   Detecci√≥n de Anomal√≠as:\")\n",
    "print(f\"    ‚îú‚îÄ Algoritmo: Isolation Forest\")\n",
    "print(f\"    ‚îú‚îÄ Anomal√≠as detectadas: {anomaly_count:,} ({anomaly_percentage:.2f}%)\")\n",
    "print(f\"    ‚îî‚îÄ Tasa m√°s alta en: {anomaly_rates.idxmax()} ({anomaly_rates.max():.2f}%)\")\n",
    "\n",
    "print(f\"\\n OPTIMIZACI√ìN DE RECURSOS:\")\n",
    "print(f\"   Per√≠odos cr√≠ticos identificados: {len(critical_periods)}\")\n",
    "if len(critical_periods) > 0:\n",
    "    worst_period = critical_periods.nlargest(1, 'hang_rate').index[0]\n",
    "    worst_day, worst_hour = worst_period\n",
    "    print(f\"   Per√≠odo m√°s cr√≠tico: D√≠a {worst_day}, Hora {worst_hour}\")\n",
    "    print(f\"    ‚îú‚îÄ Tasa de abandono: {critical_periods.loc[worst_period, 'hang_rate']:.2f}%\")\n",
    "    print(f\"    ‚îî‚îÄ Llamadas: {critical_periods.loc[worst_period, 'total_calls']:.0f}\")\n",
    "\n",
    "avg_efficiency = resource_analysis['efficiency_score'].mean()\n",
    "best_day_eff = resource_analysis.groupby('day_of_week')['efficiency_score'].mean().idxmax()\n",
    "worst_day_eff = resource_analysis.groupby('day_of_week')['efficiency_score'].mean().idxmin()\n",
    "\n",
    "print(f\"\\n EFICIENCIA OPERACIONAL:\")\n",
    "print(f\"   Score promedio de eficiencia: {avg_efficiency:.2f}\")\n",
    "print(f\"   D√≠a m√°s eficiente: {day_names[best_day_eff]}\")\n",
    "print(f\"  ‚ö†Ô∏è D√≠a menos eficiente: {day_names[worst_day_eff]}\")\n",
    "\n",
    "total_adjustments_needed = abs(resource_analysis['server_adjustment']).sum()\n",
    "print(f\"   Ajustes de personal recomendados: {total_adjustments_needed:.0f} cambios totales\")\n",
    "\n",
    "print(f\"\\n IMPACTO ESPERADO:\")\n",
    "current_hang_rate = (df['outcome'] == 'HANG').mean() * 100\n",
    "potential_improvement = 15  # Estimaci√≥n conservadora\n",
    "print(f\"   Reducci√≥n estimada de abandonos: {potential_improvement}%\")\n",
    "print(f\"    ‚îú‚îÄ Tasa actual: {current_hang_rate:.2f}%\")\n",
    "print(f\"    ‚îî‚îÄ Tasa objetivo: {current_hang_rate * (1 - potential_improvement/100):.2f}%\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "next_steps = [\n",
    "    \"1. Implementar sistema de predicci√≥n en tiempo real\",\n",
    "    \"2. Configurar alertas autom√°ticas para anomal√≠as\",\n",
    "    \"3. Desarrollar dashboard operacional\",\n",
    "    \"4. Establecer proceso de reentrenamiento mensual\",\n",
    "    \"5. Validar modelos con datos en producci√≥n\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" PROYECTO DE MACHINE LEARNING COMPLETADO\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d9b01",
   "metadata": {},
   "source": [
    "# üìñ Diccionario de Datos: call_center_clean.csv\n",
    "\n",
    "| Columna        | Tipo         | Descripci√≥n                                                                                   | Ejemplo           |\n",
    "|----------------|--------------|----------------------------------------------------------------------------------------------|-------------------|\n",
    "| vru.line       | string       | L√≠nea o canal de entrada VRU (IVR)                                                            | AA0101            |\n",
    "| call_id        | int64        | Identificador √∫nico de la llamada                                                            | 33116             |\n",
    "| customer_id    | float64      | Identificador del cliente (NaN o 0 para clientes an√≥nimos)                                   | 9664491.0         |\n",
    "| priority       | int8         | Prioridad de la llamada (0: baja, 1: media, 2: alta)                                         | 2                 |\n",
    "| type           | string       | Tipo de servicio o transacci√≥n (PS, PE, IN, NE, NW, TT)                                      | PS                |\n",
    "| date           | date         | Fecha de la llamada (YYYY-MM-DD)                                                             | 1999-01-01        |\n",
    "| vru_entry      | string (hh:mm:ss) | Hora de entrada al sistema IVR (formato HH:MM:SS)                                         | 0:00:31           |\n",
    "| vru_exit       | string (hh:mm:ss) | Hora de salida del IVR                                                                     | 0:00:36           |\n",
    "| vru_time       | int64        | Tiempo en IVR en segundos                                                                    | 5                 |\n",
    "| q_start        | string (hh:mm:ss) | Hora de inicio en cola                                                                     | 0:00:36           |\n",
    "| q_exit         | string (hh:mm:ss) | Hora de salida de la cola                                                                  | 0:03:09           |\n",
    "| q_time         | int64        | Tiempo en cola en segundos                                                                   | 153               |\n",
    "| outcome        | string       | Resultado de la llamada (AGENT: atendida, HANG: abandono, PHANTOM: fantasma)                 | HANG              |\n",
    "| ser_start      | string (hh:mm:ss) | Hora de inicio de servicio                                                                 | 0:00:00           |\n",
    "| ser_exit       | string (hh:mm:ss) | Hora de fin de servicio                                                                   | 0:00:00           |\n",
    "| ser_time       | int64        | Tiempo de servicio en segundos                                                               | 0                 |\n",
    "| server         | string       | Nombre o identificador del agente/servidor (o NO_SERVER si no fue atendida)                  | NO_SERVER         |\n",
    "| startdate      | int64        | Campo auxiliar (usualmente 0, puede indicar fecha/hora de inicio en otros formatos)           | 0                 |\n",
    "\n",
    "**Notas:**\n",
    "- Los campos de tiempo en formato string (hh:mm:ss) pueden ser \"0:00:00\" si no aplica.\n",
    "- customer_id puede ser 0 o NaN para llamadas an√≥nimas.\n",
    "- outcome define el destino final de la llamada.\n",
    "- Los tiempos (vru_time, q_time, ser_time) est√°n en segundos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}